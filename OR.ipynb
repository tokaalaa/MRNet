{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OR",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tokaalaa/MRNet/blob/master/OR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs9Cvh3ZNizB",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TFk49KOKQl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, Dropdown, IntSlider\n",
        "from tqdm import tqdm_notebook\n",
        "from __future__ import division\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import LSTM\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4UIopQSNpbX",
        "colab_type": "text"
      },
      "source": [
        "##Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRce8Nn8NWgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "d348fb17-b2a8-42d5-fbc6-4af9ab27d96d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"drive/My Drive/Colab Notebooks/MRNet-v1.0\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/MRNet-v1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQEjxqKCVZfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data generator\n",
        "class data_generator(keras.utils.Sequence):\n",
        "    def __init__(self, injury, plane, data_type):\n",
        "        self.injury = injury\n",
        "        self.plane = plane\n",
        "        self.data_type = data_type\n",
        "        if(data_type == 'train'):\n",
        "          self.labels = pd.read_csv('train-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "          self.case_list = pd.read_csv('train-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                               dtype={'case': str, 'label': np.int64})['case'].tolist()\n",
        "          self.labels = self.labels[0:1017]\n",
        "          self.case_list = self.case_list[0:1017]\n",
        "          self.data_path = 'train'\n",
        "        elif(data_type == 'valid'):\n",
        "          self.labels = pd.read_csv('train-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "          self.case_list = pd.read_csv('train-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                               dtype={'case': str, 'label': np.int64})['case'].tolist()\n",
        "          self.labels = self.labels[1017:1130]\n",
        "          self.case_list = self.case_list[1017:1130]\n",
        "          self.data_path = 'train'\n",
        "        else:\n",
        "          self.labels = pd.read_csv('valid-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "          self.case_list = pd.read_csv('valid-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                               dtype={'case': str, 'label': np.int64})['case'].tolist()\n",
        "          self.data_path = 'valid'\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.case_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fpath = '{}/{}/{}.npy'.format(self.data_path, self.plane, self.case_list[index])\n",
        "        stack = np.load(fpath).astype(float)\n",
        "        for i in range(stack.shape[0]):\n",
        "          stack[i] = stack[i]/255.\n",
        "        stack = np.repeat(stack[:, :, :, np.newaxis], 3, axis=3)\n",
        "        stack = np.expand_dims(stack, axis=0)\n",
        "\n",
        "        y = self.labels[index]\n",
        "        y = np.array(y).reshape(1,1)\n",
        "        return stack, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9Z2oKdbjfNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_output (injury, data_type):\n",
        "        if(data_type == 'train'):\n",
        "          labels = pd.read_csv('train-{}.csv'.format(injury), names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "          \n",
        "          labels = labels[0:1017]\n",
        "        \n",
        "        elif(data_type == 'valid'):\n",
        "          labels = pd.read_csv('train-{}.csv'.format(injury), names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "          \n",
        "          labels = labels[1017:1130]\n",
        "        \n",
        "        else:\n",
        "          labels = pd.read_csv('valid-{}.csv'.format(injury), names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "        \n",
        "        return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Uii0POe7K4",
        "colab_type": "text"
      },
      "source": [
        "##Combining model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0LGCoTbe_IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictor():\n",
        "  logistic_input = keras.Input(shape=(3,))\n",
        "  logistic_output = layers.Dense(1, activation='sigmoid')(logistic_input)\n",
        "\n",
        "  predictor_model = keras.Model(inputs=logistic_input, outputs=logistic_output)\n",
        "  predictor_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return predictor_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rsDrRMO9T5_",
        "colab_type": "text"
      },
      "source": [
        "##ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OasBYCwika1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # for batch normalization layer, we assume\n",
        "    # the input data is in channel last format\n",
        "    bn_axis = 3\n",
        "\n",
        "    filters1, filters2, filters3 = filters\n",
        "  \n",
        "    x = layers.Conv2D(filters1, kernel_size=(1, 1), strides=(1, 1),\n",
        "                      padding='valid', name=conv_name_base + '2a')(input_tensor)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters2, kernel_size, strides=(1, 1),\n",
        "                      padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters3, kernel_size=(1, 1), strides=(1, 1),\n",
        "                      padding='valid', name=conv_name_base + '2c')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    x = layers.add([x, input_tensor])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
        "\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    bn_axis = 3\n",
        "\n",
        "    filters1, filters2, filters3 = filters\n",
        "  \n",
        "    x = layers.Conv2D(filters1, kernel_size=(1, 1), strides=strides,\n",
        "                      padding='valid', name=conv_name_base + '2a')(input_tensor)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters2, kernel_size, strides=(1, 1),\n",
        "                      padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters3, kernel_size=(1, 1), strides=(1, 1),\n",
        "                      padding='valid', name=conv_name_base + '2c')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "    \n",
        "    # resize the input\n",
        "    shortcut = layers.Conv2D(filters3, kernel_size=(1, 1), strides=strides,\n",
        "                             padding='valid', name=conv_name_base + '1')(input_tensor)\n",
        "    shortcut = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut) \n",
        "\n",
        "    x = layers.add([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnVF-Ed5kr7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ResNet():\n",
        "   \n",
        "    img_input = keras.Input(shape=(None, 256, 256, 3))\n",
        "    n_classes = 1;\n",
        "    \n",
        "    bn_axis = 3\n",
        "    x = layers.Cropping3D(cropping=((0,0),(6,6),(6,6)))(img_input)\n",
        "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(x[0])\n",
        "    x = layers.Conv2D(64, (7, 7),\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      kernel_initializer='he_normal',\n",
        "                      name='conv1')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
        "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
        "    # print(x.shape)\n",
        "    avg = layers.GlobalAveragePooling2D()(x)\n",
        "    avg = tf.expand_dims(avg,0)\n",
        "    max_layer = layers.GlobalMaxPooling1D()(avg)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(2048, activation='relu')(max_layer)\n",
        "    img_output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs=img_input, outputs=img_output, name='resnet')\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.1)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aVqoJlct3E6o"
      },
      "source": [
        "###Abnormal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvHaMEpxPuGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "abnormal_sagittal = ResNet()\n",
        "abnormal_coronal = ResNet()\n",
        "abnormal_axial = ResNet()\n",
        "\n",
        "training_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "abnormal_sagittal.history = abnormal_sagittal.fit_generator(generator=training_abnormal_sagittal_generator, validation_data=validation_abnormal_sagittal_generator, epochs=20)\n",
        "\n",
        "abnormal_sagittal.save('resnet_abnormal_sagittal.h5')\n",
        "\n",
        "training_abnormal_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_abnormal_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "abnormal_coronal.history = abnormal_coronal.fit_generator(generator=training_abnormal_coronal_generator, validation_data=validation_abnormal_coronal_generator, epochs=20)\n",
        "\n",
        "abnormal_coronal.save('resnet_abnormal_coronal.h5')\n",
        "\n",
        "training_abnormal_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_abnormal_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "abnormal_axial.history = abnormal_axial.fit_generator(generator=training_abnormal_axial_generator, validation_data=validation_abnormal_axial_generator, epochs=20)\n",
        "\n",
        "abnormal_axial.save('resnet_abnormal_axial.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J_PoK6lQFoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "abnormal_sagittal = ResNet()\n",
        "abnormal_coronal = ResNet()\n",
        "abnormal_axial = ResNet()\n",
        "\n",
        "abnormal_sagittal.load_weights(\"resnet_abnormal_sagittal.h5\")\n",
        "abnormal_coronal.load_weights(\"resnet_abnormal_coronal.h5\")\n",
        "abnormal_axial.load_weights(\"resnet_abnormal_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCONTnTNQI2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining models\n",
        "training_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "\n",
        "training_abnormal_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_abnormal_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "\n",
        "training_abnormal_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_abnormal_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = abnormal_sagittal.predict_generator(training_abnormal_sagittal_generator)\n",
        "training_coronal_prediction = abnormal_coronal.predict_generator(training_abnormal_coronal_generator)\n",
        "training_axial_prediction = abnormal_axial.predict_generator(training_abnormal_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = abnormal_sagittal.predict_generator(validation_abnormal_sagittal_generator)\n",
        "validation_coronal_prediction = abnormal_coronal.predict_generator(validation_abnormal_coronal_generator)\n",
        "validation_axial_prediction = abnormal_axial.predict_generator(validation_abnormal_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('abnormal','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('abnormal','valid'))\n",
        "\n",
        "abnormal_model = predictor()\n",
        "abnormal_model.hisory = abnormal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "abnormal_model.save('combine_resnet_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olgwmKZ6SsqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "abnormal_model = predictor()\n",
        "abnormal_model.load_weights('combine_resnet_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LHuQ5PuaYQvS",
        "colab": {}
      },
      "source": [
        "# testing the model\n",
        "test_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal', 'test')\n",
        "test_abnormal_coronal_generator = data_generator('abnormal', 'coronal', 'test')\n",
        "test_abnormal_axial_generator = data_generator('abnormal', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = abnormal_sagittal.predict_generator(test_abnormal_sagittal_generator)\n",
        "test_coronal_prediction = abnormal_coronal.predict_generator(test_abnormal_coronal_generator)\n",
        "test_axial_prediction = abnormal_axial.predict_generator(test_abnormal_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('abnormal','test'))\n",
        "\n",
        "abnormal_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YTIjzZ7Q3E-f"
      },
      "source": [
        "###ACL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJgJr6vXPtnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "acl_sagittal = ResNet()\n",
        "acl_coronal = ResNet()\n",
        "acl_axial = ResNet()\n",
        "\n",
        "training_acl_sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_acl_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "acl_sagittal.history = acl_sagittal.fit_generator(generator=training_acl_sagittal_generator, validation_data=validation_acl_sagittal_generator, epochs=20)\n",
        "\n",
        "acl_sagittal.save('resnet_acl_sagittal.h5')\n",
        "\n",
        "training_acl_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_acl_axial_generator = data_generator('acl', 'axial','valid')\n",
        "acl_axial.history = acl_axial.fit_generator(generator=training_acl_axial_generator, validation_data=validation_acl_axial_generator, epochs=20)\n",
        "\n",
        "acl_axial.save('resnet_acl_axial.h5')\n",
        "\n",
        "training_acl_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_acl_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "acl_coronal.history = acl_coronal.fit_generator(generator=training_acl_coronal_generator, validation_data=validation_acl_coronal_generator, epochs=20)\n",
        "\n",
        "acl_coronal.save('resnet_acl_coronal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw1bVtBqQN-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "acl_sagittal = ResNet()\n",
        "acl_coronal = ResNet()\n",
        "acl_axial = ResNet()\n",
        "\n",
        "acl_sagittal.load_weights(\"resnet_acl_sagittal.h5\")\n",
        "acl_coronal.load_weights(\"resnet_acl_coronal.h5\")\n",
        "acl_axial.load_weights(\"resnet_acl_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIVAX26YQSSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining models\n",
        "training_acl_sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_acl_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "\n",
        "training_acl_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_acl_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "\n",
        "training_acl_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_acl_axial_generator = data_generator('acl', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = acl_sagittal.predict_generator(training_acl_sagittal_generator)\n",
        "training_coronal_prediction = acl_coronal.predict_generator(training_acl_coronal_generator)\n",
        "training_axial_prediction = acl_axial.predict_generator(training_acl_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = acl_sagittal.predict_generator(validation_acl_sagittal_generator)\n",
        "validation_coronal_prediction = acl_coronal.predict_generator(validation_acl_coronal_generator)\n",
        "validation_axial_prediction = acl_axial.predict_generator(validation_acl_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('acl','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('acl','valid'))\n",
        "\n",
        "acl_model = predictor()\n",
        "acl_model.hisory = acl_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "acl_model.save('combine_resnet_acl.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjWAWC_JS02r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "acl_model = predictor()\n",
        "acl_model.load_weights('combine_resnet_acl.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zbbd1lHPYY-A",
        "colab": {}
      },
      "source": [
        "# testing the model\n",
        "test_acl_sagittal_generator = data_generator('acl', 'sagittal', 'test')\n",
        "test_acl_coronal_generator = data_generator('acl', 'coronal', 'test')\n",
        "test_acl_axial_generator = data_generator('acl', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = acl_sagittal.predict_generator(test_acl_sagittal_generator)\n",
        "test_coronal_prediction = acl_coronal.predict_generator(test_acl_coronal_generator)\n",
        "test_axial_prediction = acl_axial.predict_generator(test_acl_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('acl','test'))\n",
        "\n",
        "acl_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gEnBElSW3FCJ"
      },
      "source": [
        "###Meniscus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOXC6hYgPs9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "meniscus_sagittal = ResNet()\n",
        "meniscus_coronal = ResNet()\n",
        "meniscus_axial = ResNet()\n",
        "\n",
        "training_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "meniscus_sagittal.history = meniscus_sagittal.fit_generator(generator=training_meniscus_sagittal_generator, validation_data=validation_meniscus_sagittal_generator, epochs=20)\n",
        "\n",
        "meniscus_sagittal.save('resnet_meniscus_sagittal.h5')\n",
        "\n",
        "training_meniscus_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_meniscus_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "meniscus_axial.history = meniscus_axial.fit_generator(generator=training_meniscus_axial_generator, validation_data=validation_meniscus_axial_generator, epochs=20)\n",
        "\n",
        "meniscus_axial.save('resnet_meniscus_axial.h5')\n",
        "\n",
        "training_meniscus_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_meniscus_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "meniscus_coronal.history = meniscus_coronal.fit_generator(generator=training_meniscus_coronal_generator, validation_data=validation_meniscus_coronal_generator, epochs=20)\n",
        "\n",
        "meniscus_coronal.save('resnet_meniscus_coronal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ_h62CYQXbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "meniscus_sagittal = ResNet()\n",
        "meniscus_coronal = ResNet()\n",
        "meniscus_axial = ResNet()\n",
        "\n",
        "meniscus_sagittal.load_weights(\"resnet_meniscus_sagittal.h5\")\n",
        "meniscus_coronal.load_weights(\"resnet_meniscus_coronal.h5\")\n",
        "meniscus_axial.load_weights(\"resnet_meniscus_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mahas6vJQZrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining models\n",
        "training_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "\n",
        "training_meniscus_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_meniscus_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "\n",
        "training_meniscus_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_meniscus_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = meniscus_sagittal.predict_generator(training_meniscus_sagittal_generator)\n",
        "training_coronal_prediction = meniscus_coronal.predict_generator(training_meniscus_coronal_generator)\n",
        "training_axial_prediction = meniscus_axial.predict_generator(training_meniscus_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = meniscus_sagittal.predict_generator(validation_meniscus_sagittal_generator)\n",
        "validation_coronal_prediction = meniscus_coronal.predict_generator(validation_meniscus_coronal_generator)\n",
        "validation_axial_prediction = meniscus_axial.predict_generator(validation_meniscus_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('meniscus','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('meniscus','valid'))\n",
        "\n",
        "meniscus_model = predictor()\n",
        "meniscus_model.hisory = meniscus_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "meniscus_model.save('combine_resnet_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAdlsE4WS76e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "meniscus_model = predictor()\n",
        "meniscus_model.load_weights('combine_resnet_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M9sF7u0vZFCJ",
        "colab": {}
      },
      "source": [
        "# testing the model\n",
        "test_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal', 'test')\n",
        "test_meniscus_coronal_generator = data_generator('meniscus', 'coronal', 'test')\n",
        "test_meniscus_axial_generator = data_generator('meniscus', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = meniscus_sagittal.predict_generator(test_meniscus_sagittal_generator)\n",
        "test_coronal_prediction = meniscus_coronal.predict_generator(test_meniscus_coronal_generator)\n",
        "test_axial_prediction = meniscus_axial.predict_generator(test_meniscus_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('meniscus','test'))\n",
        "\n",
        "meniscus_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNbJjCV-sAsZ",
        "colab_type": "text"
      },
      "source": [
        "##VGG Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZNLHrTd3wgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for creating a vgg block\n",
        "def vgg_block(layer_in, n_filters, n_conv):\n",
        "\t# add convolutional layers\n",
        "\tfor _ in range(n_conv):\n",
        "\t\tlayer_in = layers.Conv2D(n_filters, (3,3), padding='same', activation='relu')(layer_in)\n",
        "\t# add max pooling layer\n",
        "\tlayer_in = layers.MaxPooling2D((2,2), strides=(2,2))(layer_in)\n",
        "\treturn layer_in\n",
        " \n",
        "# define model input\n",
        "def vgg_model():\n",
        "\tvisible = keras.Input(shape=(None, 256, 256, 3))\n",
        "\tlayer = vgg_block(visible[0], 64, 2)\n",
        "\tlayer = vgg_block(layer, 128, 2)\n",
        "\tlayer = vgg_block(layer, 256, 3)\n",
        "\tlayer = vgg_block(layer, 512, 3)\n",
        "\tlayer = vgg_block(layer, 512, 3)\n",
        "\n",
        "\tavg = layers.GlobalAveragePooling2D()(layer)\n",
        "\tavg = tf.expand_dims(avg,0)\n",
        "\tmax_layer = layers.GlobalMaxPooling1D()(avg)\n",
        "\tlayer = layers.Dense(512, activation='relu')(max_layer)\n",
        "\tout = layers.Dense(1, activation='sigmoid')(layer)\n",
        "\n",
        "\tvgg_model = models.Model(inputs=visible, outputs=out)\n",
        "\tvgg_model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\treturn vgg_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3KKNOM1Z3aHw"
      },
      "source": [
        "###Abnormal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuEMIFhz7kmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "abnormal_sagittal = vgg_model()\n",
        "abnormal_coronal = vgg_model()\n",
        "abnormal_axial = vgg_model()\n",
        "\n",
        "training_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "abnormal_sagittal.history = abnormal_sagittal.fit_generator(generator=training_abnormal_sagittal_generator, validation_data=validation_abnormal_sagittal_generator, epochs=5)\n",
        "\n",
        "abnormal_sagittal.save('vgg_abnormal_sagittal.h5')\n",
        "\n",
        "training_abnormal_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_abnormal_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "abnormal_coronal.history = abnormal_coronal.fit_generator(generator=training_abnormal_coronal_generator, validation_data=validation_abnormal_coronal_generator, epochs=5)\n",
        "\n",
        "abnormal_coronal.save('vgg_abnormal_coronal.h5')\n",
        "\n",
        "training_abnormal_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_abnormal_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "abnormal_axial.history = abnormal_axial.fit_generator(generator=training_abnormal_axial_generator, validation_data=validation_abnormal_axial_generator, epochs=5)\n",
        "\n",
        "abnormal_axial.save('vgg_abnormal_axial.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5lXMIdndP5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "abnormal_sagittal = vgg_model()\n",
        "abnormal_coronal = vgg_model()\n",
        "abnormal_axial = vgg_model()\n",
        "\n",
        "abnormal_sagittal.load_weights(\"vgg_abnormal_sagittal.h5\")\n",
        "abnormal_coronal.load_weights(\"vgg_abnormal_coronal.h5\")\n",
        "abnormal_axial.load_weights(\"vgg_abnormal_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRiexeebdPif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining models\n",
        "training_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "\n",
        "training_abnormal_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_abnormal_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "\n",
        "training_abnormal_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_abnormal_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = abnormal_sagittal.predict_generator(training_abnormal_sagittal_generator)\n",
        "training_coronal_prediction = abnormal_coronal.predict_generator(training_abnormal_coronal_generator)\n",
        "training_axial_prediction = abnormal_axial.predict_generator(training_abnormal_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = abnormal_sagittal.predict_generator(validation_abnormal_sagittal_generator)\n",
        "validation_coronal_prediction = abnormal_coronal.predict_generator(validation_abnormal_coronal_generator)\n",
        "validation_axial_prediction = abnormal_axial.predict_generator(validation_abnormal_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('abnormal','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('abnormal','valid'))\n",
        "\n",
        "abnormal_model = predictor()\n",
        "abnormal_model.hisory = abnormal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "abnormal_model.save('combine_vgg_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhJ5lOxBkbIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "abnormal_model = predictor()\n",
        "abnormal_model.load_weights('combine_vgg_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhWd4Ox9kaxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing the model\n",
        "test_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal', 'test')\n",
        "test_abnormal_coronal_generator = data_generator('abnormal', 'coronal', 'test')\n",
        "test_abnormal_axial_generator = data_generator('abnormal', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = abnormal_sagittal.predict_generator(test_abnormal_sagittal_generator)\n",
        "test_coronal_prediction = abnormal_coronal.predict_generator(test_abnormal_coronal_generator)\n",
        "test_axial_prediction = abnormal_axial.predict_generator(test_abnormal_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('abnormal','test'))\n",
        "\n",
        "abnormal_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t6TT-O9Z3aSQ"
      },
      "source": [
        "###ACL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtwwggSoPq2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "acl_sagittal = vgg_model()\n",
        "acl_coronal = vgg_model()\n",
        "acl_axial = vgg_model()\n",
        "\n",
        "training_acl_sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_acl_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "acl_sagittal.history = acl_sagittal.fit_generator(generator=training_acl_sagittal_generator, validation_data=validation_acl_sagittal_generator, epochs=5)\n",
        "\n",
        "acl_sagittal.save('vgg_acl_sagittal.h5')\n",
        "\n",
        "training_acl_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_acl_axial_generator = data_generator('acl', 'axial','valid')\n",
        "acl_axial.history = acl_axial.fit_generator(generator=training_acl_axial_generator, validation_data=validation_acl_axial_generator, epochs=5)\n",
        "\n",
        "acl_axial.save('vgg_acl_axial.h5')\n",
        "\n",
        "training_acl_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_acl_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "acl_coronal.history = acl_coronal.fit_generator(generator=training_acl_coronal_generator, validation_data=validation_acl_coronal_generator, epochs=5)\n",
        "\n",
        "acl_coronal.save('vgg_acl_coronal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOn7xkT8dhc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acl_sagittal = vgg_model()\n",
        "acl_coronal = vgg_model()\n",
        "acl_axial = vgg_model()\n",
        "\n",
        "acl_sagittal.load_weights(\"vgg_acl_sagittal.h5\")\n",
        "acl_coronal.load_weights(\"vgg_acl_coronal.h5\")\n",
        "acl_axial.load_weights(\"vgg_acl_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6nS42SndhFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining models\n",
        "training_acl_sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_acl_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "\n",
        "training_acl_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_acl_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "\n",
        "training_acl_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_acl_axial_generator = data_generator('acl', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = acl_sagittal.predict_generator(training_acl_sagittal_generator)\n",
        "training_coronal_prediction = acl_coronal.predict_generator(training_acl_coronal_generator)\n",
        "training_axial_prediction = acl_axial.predict_generator(training_acl_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = acl_sagittal.predict_generator(validation_acl_sagittal_generator)\n",
        "validation_coronal_prediction = acl_coronal.predict_generator(validation_acl_coronal_generator)\n",
        "validation_axial_prediction = acl_axial.predict_generator(validation_acl_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('acl','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('acl','valid'))\n",
        "\n",
        "acl_model = predictor()\n",
        "acl_model.hisory = acl_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "acl_model.save('combine_vgg_acl.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJvl-tSgklD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "acl_model = predictor()\n",
        "acl_model.load_weights('combine_vgg_acl.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nj9T0lakkvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "17a27fb1-ef30-4478-a8ca-97bfeda117e9"
      },
      "source": [
        "# testing the model\n",
        "test_acl_sagittal_generator = data_generator('acl', 'sagittal', 'test')\n",
        "test_acl_coronal_generator = data_generator('acl', 'coronal', 'test')\n",
        "test_acl_axial_generator = data_generator('acl', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = acl_sagittal.predict_generator(test_acl_sagittal_generator)\n",
        "test_coronal_prediction = acl_coronal.predict_generator(test_acl_coronal_generator)\n",
        "test_axial_prediction = acl_axial.predict_generator(test_acl_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('acl','test'))\n",
        "\n",
        "acl_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 2ms/step - loss: 0.8610 - accuracy: 0.5500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.861045241355896, 0.550000011920929]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mmdRbFbR3aWQ"
      },
      "source": [
        "###Meniscus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sutb_z1GeQO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "meniscus_sagittal = vgg_model()\n",
        "meniscus_coronal = vgg_model()\n",
        "meniscus_axial = vgg_model()\n",
        "\n",
        "training_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "meniscus_axial.history = meniscus_sagittal.fit_generator(generator=training_meniscus_sagittal_generator, validation_data=validation_meniscus_sagittal_generator, epochs=5)\n",
        "\n",
        "meniscus_sagittal.save('vgg_meniscus_sagittal.h5')\n",
        "\n",
        "training_meniscus_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_meniscus_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "meniscus_axial.history = meniscus_axial.fit_generator(generator=training_meniscus_axial_generator, validation_data=validation_meniscus_axial_generator, epochs=5)\n",
        "\n",
        "meniscus_axial.save('vgg_meniscus_axial.h5')\n",
        "\n",
        "training_meniscus_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_meniscus_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "meniscus_coronal.history = meniscus_coronal.fit_generator(generator=training_meniscus_coronal_generator, validation_data=validation_meniscus_coronal_generator, epochs=5)\n",
        "\n",
        "meniscus_coronal.save('vgg_meniscus_coronal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Il0meveeQAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "meniscus_sagittal = vgg_model()\n",
        "meniscus_coronal = vgg_model()\n",
        "meniscus_axial = vgg_model()\n",
        "\n",
        "meniscus_sagittal.load_weights(\"vgg_meniscus_sagittal.h5\")\n",
        "meniscus_coronal.load_weights(\"vgg_meniscus_coronal.h5\")\n",
        "meniscus_axial.load_weights(\"vgg_meniscus_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrsYNZv1PqUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining models\n",
        "training_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "\n",
        "training_meniscus_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_meniscus_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "\n",
        "training_meniscus_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_meniscus_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = meniscus_sagittal.predict_generator(training_meniscus_sagittal_generator)\n",
        "training_coronal_prediction = meniscus_coronal.predict_generator(training_meniscus_coronal_generator)\n",
        "training_axial_prediction = meniscus_axial.predict_generator(training_meniscus_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = meniscus_sagittal.predict_generator(validation_meniscus_sagittal_generator)\n",
        "validation_coronal_prediction = meniscus_coronal.predict_generator(validation_meniscus_coronal_generator)\n",
        "validation_axial_prediction = meniscus_axial.predict_generator(validation_meniscus_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('meniscus','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('meniscus','valid'))\n",
        "\n",
        "meniscus_model = predictor()\n",
        "meniscus_model.hisory = meniscus_model.fit(training_input, training_output,epochs=100,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "meniscus_model.save('combine_vgg_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dokyMT6kmkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "meniscus_model = predictor()\n",
        "meniscus_model.load_weights('combine_vgg_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTY_9op0kmP4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d8f7072c-b614-4130-e793-53e5eb754c28"
      },
      "source": [
        "# testing the model\n",
        "test_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal', 'test')\n",
        "test_meniscus_coronal_generator = data_generator('meniscus', 'coronal', 'test')\n",
        "test_meniscus_axial_generator = data_generator('meniscus', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = meniscus_sagittal.predict_generator(test_meniscus_sagittal_generator)\n",
        "test_coronal_prediction = meniscus_coronal.predict_generator(test_meniscus_coronal_generator)\n",
        "test_axial_prediction = meniscus_axial.predict_generator(test_meniscus_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('meniscus','test'))\n",
        "\n",
        "meniscus_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 2ms/step - loss: 0.6980 - accuracy: 0.5667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6979799270629883, 0.5666666626930237]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwUlr0IC9eOi",
        "colab_type": "text"
      },
      "source": [
        "##Inception Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvZ16_oE9i7G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c869e4b-b7d9-45bc-bf5d-87cffb3e21d2"
      },
      "source": [
        "def BNConv(nb_filter, nb_row, nb_col, strides=(1, 1, 1), padding=\"valid\"):\n",
        "    def f(input):\n",
        "        conv = layers.Conv3D(filters=nb_filter, kernel_size=(1,nb_row,nb_col), strides=strides,\n",
        "                      padding=padding)(input)\n",
        "        x = layers.BatchNormalization(axis=2)(conv)\n",
        "        return layers.Activation('relu')(x) #modified apply batchNormalization then activation\n",
        "    return f\n",
        "\n",
        "def InceptionFig5(filters):\n",
        "    def f(input):\n",
        "\n",
        "        # Tower A\n",
        "        conv_a1 = BNConv(64, 1, 1, padding=\"same\")(input)\n",
        "        conv_a2 = BNConv(96, 3, 3, padding=\"same\")(conv_a1)\n",
        "        conv_a3 = BNConv(96, 3, 3, padding=\"same\")(conv_a2)\n",
        "\n",
        "        # Tower B\n",
        "        conv_b1 = BNConv(48, 1, 1, padding=\"same\")(input)\n",
        "        conv_b2 = BNConv(64, 3, 3, padding=\"same\")(conv_b1)\n",
        "\n",
        "        # Tower C\n",
        "        pool_c1 = layers.AveragePooling3D(pool_size=(1, 3, 3), strides=(1, 1, 1), padding=\"same\")(input)\n",
        "        conv_c2 = BNConv(filters, 1, 1, padding=\"same\")(pool_c1)#modified\n",
        "\n",
        "        # Tower D\n",
        "        conv_d1 = BNConv(64, 1, 1, padding=\"same\")(input)\n",
        "\n",
        "        return keras.layers.concatenate([conv_a3, conv_b2, conv_c2, conv_d1], axis=4)\n",
        "\n",
        "    return f\n",
        "\n",
        "def DimReductionA():\n",
        "    def f(input):\n",
        "        conv_a1 = BNConv(64, 1, 1, padding=\"same\")(input)\n",
        "        conv_a2 = BNConv(96, 3, 3, padding=\"same\")(conv_a1)\n",
        "        conv_a3 = BNConv(96, 3, 3, strides=(1, 2, 2), padding=\"valid\")(conv_a2)\n",
        "\n",
        "        # another inconsistency between (model.txt + mxnet diagram) and (the paper)\n",
        "        # the Fig 10 in the paper shows a 1x1 convolution before\n",
        "        # the 3x3. Going with model.txt + mxnet diagram\n",
        "        conv_b = BNConv(384, 3, 3, strides=(1, 2, 2), padding =\"valid\")(input)\n",
        "\n",
        "        pool_c = layers.MaxPooling3D(pool_size=(1, 3, 3), strides=(1, 2, 2), padding=\"valid\")(input)\n",
        "\n",
        "        return keras.layers.concatenate([conv_a3, conv_b, pool_c], axis=4)\n",
        "    return f\n",
        "\n",
        "def InceptionFig6(filters):\n",
        "    def f(input):\n",
        "        conv_a1 = BNConv(filters, 1, 1, padding=\"same\")(input)\n",
        "        conv_a2 = BNConv(filters, 1, 7, padding=\"same\")(conv_a1)\n",
        "        conv_a3 = BNConv(filters, 7, 1, padding=\"same\")(conv_a2)\n",
        "        conv_a4 = BNConv(filters, 1, 7, padding=\"same\")(conv_a3)\n",
        "        conv_a5 = BNConv(192, 7, 1, padding=\"same\")(conv_a4)\n",
        "\n",
        "        # Tower B\n",
        "        conv_b1 = BNConv(filters, 1, 1, padding=\"same\")(input)\n",
        "        conv_b2 = BNConv(filters, 1, 7, padding=\"same\")(conv_b1)\n",
        "        conv_b3 = BNConv(192, 7, 1, padding=\"same\")(conv_b2)\n",
        "\n",
        "        # Tower C\n",
        "        pool_c1 = layers.AveragePooling3D(pool_size=(1, 3, 3), strides=(1, 1, 1), padding=\"same\")(input)\n",
        "        conv_c2 = BNConv(192, 1, 1)(pool_c1)\n",
        "\n",
        "        # Tower D\n",
        "        conv_d = BNConv(192, 1, 1)(input)\n",
        "\n",
        "        return keras.layers.concatenate([conv_a5, conv_b3, conv_c2, conv_d], axis=4)\n",
        "\n",
        "    return f\n",
        "\n",
        "def DimReductionB():\n",
        "    def f(input):\n",
        "        # Tower A\n",
        "        conv_a1 = BNConv(192, 1, 1)(input)\n",
        "        conv_a2 = BNConv(320, 3, 3, strides=(1, 2, 2), padding=\"valid\")(conv_a1)\n",
        "\n",
        "        # Tower B\n",
        "        conv_b1 = BNConv(192, 1, 1, padding=\"same\")(input)\n",
        "        conv_b2 = BNConv(192, 1, 7, padding=\"same\")(conv_b1)\n",
        "        conv_b3 = BNConv(192, 7, 1, padding=\"same\")(conv_b2)\n",
        "        conv_b4 = BNConv(192, 3, 3, strides=(1, 2, 2), padding=\"valid\")(conv_b3)\n",
        "\n",
        "        # Tower C\n",
        "        pool_c = layers.MaxPooling3D(pool_size=(1, 3, 3), strides=(1, 2, 2), padding=\"valid\")(input)\n",
        "\n",
        "        return keras.layers.concatenate([conv_a2, conv_b4, pool_c], axis=4)\n",
        "    return f\n",
        "\n",
        "def InceptionFig7():\n",
        "    def f(input):\n",
        "        # Tower A\n",
        "        conv_a1 = BNConv(448, 1, 1, padding=\"same\")(input)\n",
        "        conv_a2 = BNConv(384, 3, 3, padding=\"same\")(conv_a1)\n",
        "        conv_a3 = BNConv(384, 1, 3, padding=\"same\")(conv_a2)\n",
        "        conv_a4 = BNConv(384, 3, 1, padding=\"same\")(conv_a2)#modified conv_a2 instead of conv_a3\n",
        "\n",
        "        # Tower B\n",
        "        conv_b1 = BNConv(384, 1, 1, padding=\"same\")(input)\n",
        "        conv_b2 = BNConv(384, 1, 3, padding=\"same\")(conv_b1)\n",
        "        conv_b3 = BNConv(384, 3, 1, padding=\"same\")(conv_b1)#modified conv_b1 instead of conv_b2\n",
        "\n",
        "        # Tower C\n",
        "        pool_c1 = layers.AveragePooling3D(pool_size=(1, 3, 3), strides=(1, 1, 1), padding=\"same\")(input)\n",
        "        conv_c2 = BNConv(192, 1, 1)(pool_c1)\n",
        "\n",
        "        # Tower D\n",
        "        conv_d = BNConv(320, 1, 1)(input)\n",
        "\n",
        "        return keras.layers.concatenate([conv_a4, conv_a3, conv_b3, conv_b2, conv_c2, conv_d], axis=4)#modified\n",
        "\n",
        "    return f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KsgaDEx0s44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inception_v3():    \n",
        "    input = keras.Input(shape=(None,256, 256, 3))\n",
        "    conv_1 = BNConv(32, 3, 3, strides=(1, 2, 2))(input)\n",
        "    conv_2 = BNConv(32, 3, 3)(conv_1)\n",
        "    conv_3 = BNConv(64, 3, 3, padding=\"same\")(conv_2)\n",
        "    pool_4 = layers.MaxPooling3D(pool_size=(1, 3, 3), strides=(1, 2, 2), padding=\"valid\")(conv_3)\n",
        "    conv_5 = BNConv(80, 1, 1)(pool_4)\n",
        "    conv_6 = BNConv(192, 3, 3)(conv_5)\n",
        "    pool_7 = layers.MaxPooling3D(pool_size=(1,3, 3), strides=(1,2, 2), padding=\"valid\")(conv_6)\n",
        "    inception_8 = InceptionFig5(filters = 32)(pool_7)\n",
        "    inception_9 = InceptionFig5(filters = 64)(inception_8)\n",
        "    inception_10 = InceptionFig5(filters = 64)(inception_9)\n",
        "    inception_11 = DimReductionA()(inception_10)\n",
        "    inception_12 = InceptionFig6(128)(inception_11)\n",
        "    inception_13 = InceptionFig6(160)(inception_12)\n",
        "    inception_14 = InceptionFig6(160)(inception_13)\n",
        "    inception_15 = InceptionFig6(192)(inception_14)\n",
        "    inception_17 = DimReductionB()(inception_15)\n",
        "    inception_18 = InceptionFig7()(inception_17)\n",
        "    inception_19 = InceptionFig7()(inception_18)\n",
        "    #**** modified *****\n",
        "    x = layers.AveragePooling3D(strides=(1,6,6))(inception_19)\n",
        "    x = layers.GlobalMaxPooling3D()(x)\n",
        "    pooling_output = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = keras.Model(inputs= input, outputs=pooling_output, name = 'inception_v3')\n",
        "    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYBgBbW525Z7",
        "colab_type": "text"
      },
      "source": [
        "###Abnormal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZssGiQ5Ppfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "abnormal_sagittal = inception_v3()\n",
        "abnormal_coronal = inception_v3()\n",
        "abnormal_axial = inception_v3()\n",
        "\n",
        "training_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "abnormal_sagittal.history = abnormal_sagittal.fit_generator(generator=training_abnormal_sagittal_generator, validation_data=validation_abnormal_sagittal_generator, epochs=1)\n",
        "\n",
        "abnormal_sagittal.save('inception_abnormal_sagittal.h5')\n",
        "\n",
        "training_abnormal_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_abnormal_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "abnormal_coronal.history = abnormal_coronal.fit_generator(generator=training_abnormal_coronal_generator, validation_data=validation_abnormal_coronal_generator, epochs=1)\n",
        "\n",
        "abnormal_coronal.save('inception_abnormal_coronal.h5')\n",
        "\n",
        "training_abnormal_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_abnormal_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "abnormal_axial.history = abnormal_axial.fit_generator(generator=training_abnormal_axial_generator, validation_data=validation_abnormal_axial_generator, epochs=1)\n",
        "\n",
        "abnormal_axial.save('inception_abnormal_axial.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWW6nZtOQ8f3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "abnormal_sagittal = inception_v3()\n",
        "abnormal_coronal = inception_v3()\n",
        "abnormal_axial = inception_v3()\n",
        "\n",
        "abnormal_sagittal.load_weights(\"inception_abnormal_sagittal.h5\")\n",
        "abnormal_coronal.load_weights(\"inception_abnormal_coronal.h5\")\n",
        "abnormal_axial.load_weights(\"inception_abnormal_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNm-_I1rQ_Lq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining models\n",
        "training_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "\n",
        "training_abnormal_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_abnormal_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "\n",
        "training_abnormal_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_abnormal_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = abnormal_sagittal.predict_generator(training_abnormal_sagittal_generator)\n",
        "training_coronal_prediction = abnormal_coronal.predict_generator(training_abnormal_coronal_generator)\n",
        "training_axial_prediction = abnormal_axial.predict_generator(training_abnormal_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = abnormal_sagittal.predict_generator(validation_abnormal_sagittal_generator)\n",
        "validation_coronal_prediction = abnormal_coronal.predict_generator(validation_abnormal_coronal_generator)\n",
        "validation_axial_prediction = abnormal_axial.predict_generator(validation_abnormal_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('abnormal','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('abnormal','valid'))\n",
        "\n",
        "abnormal_model = predictor()\n",
        "abnormal_model.hisory = abnormal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "abnormal_model.save('combine_inception_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YYY7IbsTHby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "abnormal_model = predictor()\n",
        "abnormal_model.load_weights('combine_inception_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lZnRXowmZbb4",
        "colab": {}
      },
      "source": [
        "# testing the model\n",
        "test_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal', 'test')\n",
        "test_abnormal_coronal_generator = data_generator('abnormal', 'coronal', 'test')\n",
        "test_abnormal_axial_generator = data_generator('abnormal', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = abnormal_sagittal.predict_generator(test_abnormal_sagittal_generator)\n",
        "test_coronal_prediction = abnormal_coronal.predict_generator(test_abnormal_coronal_generator)\n",
        "test_axial_prediction = abnormal_axial.predict_generator(test_abnormal_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('abnormal','test'))\n",
        "\n",
        "abnormal_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F2cHWMpg2_2_"
      },
      "source": [
        "###ACL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOktPx9OPo8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "acl_sagittal = inception_v3()\n",
        "acl_coronal = inception_v3()\n",
        "acl_axial = inception_v3()\n",
        "\n",
        "training_acl_sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_acl_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "acl_sagittal.history = acl_sagittal.fit_generator(generator=training_acl_sagittal_generator, validation_data=validation_acl_sagittal_generator, epochs=1)\n",
        "\n",
        "acl_sagittal.save('inception_acl_sagittal.h5')\n",
        "\n",
        "training_acl_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_acl_axial_generator = data_generator('acl', 'axial','valid')\n",
        "acl_axial.history = acl_axial.fit_generator(generator=training_acl_axial_generator, validation_data=validation_acl_axial_generator, epochs=1)\n",
        "\n",
        "acl_axial.save('inception_acl_axial.h5')\n",
        "\n",
        "training_acl_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_acl_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "acl_coronal.history = acl_coronal.fit_generator(generator=training_acl_coronal_generator, validation_data=validation_acl_coronal_generator, epochs=1)\n",
        "\n",
        "acl_coronal.save('inception_acl_coronal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jdUYhqfRHeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "acl_sagittal = inception_v3()\n",
        "acl_coronal = inception_v3()\n",
        "acl_axial = inception_v3()\n",
        "\n",
        "acl_sagittal.load_weights(\"inception_acl_sagittal.h5\")\n",
        "acl_coronal.load_weights(\"inception_acl_coronal.h5\")\n",
        "acl_axial.load_weights(\"inception_acl_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syQc-l7gRIJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining models\n",
        "training_acl_sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_acl_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "\n",
        "training_acl_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_acl_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "\n",
        "training_acl_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_acl_axial_generator = data_generator('acl', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = acl_sagittal.predict_generator(training_acl_sagittal_generator)\n",
        "training_coronal_prediction = acl_coronal.predict_generator(training_acl_coronal_generator)\n",
        "training_axial_prediction = acl_axial.predict_generator(training_acl_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = acl_sagittal.predict_generator(validation_acl_sagittal_generator)\n",
        "validation_coronal_prediction = acl_coronal.predict_generator(validation_acl_coronal_generator)\n",
        "validation_axial_prediction = acl_axial.predict_generator(validation_acl_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('acl','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('acl','valid'))\n",
        "\n",
        "acl_model = predictor()\n",
        "acl_model.hisory = acl_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "meniscus_model.save('combine_inception_acl.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AZGEs3kTdW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "acl_model = predictor()\n",
        "acl_model.load_weights('combine_inception_acl.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tI14I7NUZSSp",
        "colab": {}
      },
      "source": [
        "# testing the model\n",
        "test_acl_sagittal_generator = data_generator('acl', 'sagittal', 'test')\n",
        "test_acl_coronal_generator = data_generator('acl', 'coronal', 'test')\n",
        "test_acl_axial_generator = data_generator('acl', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = acl_sagittal.predict_generator(test_acl_sagittal_generator)\n",
        "test_coronal_prediction = acl_coronal.predict_generator(test_acl_coronal_generator)\n",
        "test_axial_prediction = acl_axial.predict_generator(test_acl_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('acl','test'))\n",
        "\n",
        "acl_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YDswKVgo3AAv"
      },
      "source": [
        "###Meniscus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4HsRkiLPoYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "meniscus_sagittal = inception_v3()\n",
        "meniscus_coronal = inception_v3()\n",
        "meniscus_axial = inception_v3()\n",
        "\n",
        "training_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "meniscus_sagittal.history = meniscus_sagittal.fit_generator(generator=training_meniscus_sagittal_generator, validation_data=validation_meniscus_sagittal_generator, epochs=1)\n",
        "\n",
        "meniscus_sagittal.save('inception_meniscus_sagittal.h5')\n",
        "\n",
        "training_meniscus_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_meniscus_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "meniscus_axial.history = meniscus_axial.fit_generator(generator=training_meniscus_axial_generator, validation_data=validation_meniscus_axial_generator, epochs=1)\n",
        "\n",
        "meniscus_axial.save('inception_meniscus_axial.h5')\n",
        "\n",
        "training_meniscus_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_meniscus_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "meniscus_coronal.history = meniscus_coronal.fit_generator(generator=training_meniscus_coronal_generator, validation_data=validation_meniscus_coronal_generator, epochs=1)\n",
        "\n",
        "meniscus_coronal.save('inception_meniscus_coronal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajpasqUARN2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "meniscus_sagittal = inception_v3()\n",
        "meniscus_coronal = inception_v3()\n",
        "meniscus_axial = inception_v3()\n",
        "\n",
        "meniscus_sagittal.load_weights(\"inception_meniscus_sagittal.h5\")\n",
        "meniscus_coronal.load_weights(\"inception_meniscus_coronal.h5\")\n",
        "meniscus_axial.load_weights(\"inception_meniscus_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMO_SwSJRREt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combining models\n",
        "training_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "\n",
        "training_meniscus_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_meniscus_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "\n",
        "training_meniscus_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_meniscus_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = meniscus_sagittal.predict_generator(training_meniscus_sagittal_generator)\n",
        "training_coronal_prediction = meniscus_coronal.predict_generator(training_meniscus_coronal_generator)\n",
        "training_axial_prediction = meniscus_axial.predict_generator(training_meniscus_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = meniscus_sagittal.predict_generator(validation_meniscus_sagittal_generator)\n",
        "validation_coronal_prediction = meniscus_coronal.predict_generator(validation_meniscus_coronal_generator)\n",
        "validation_axial_prediction = meniscus_axial.predict_generator(validation_meniscus_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('meniscus','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('meniscus','valid'))\n",
        "\n",
        "meniscus_model = predictor()\n",
        "meniscus_model.hisory = meniscus_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "meniscus_model.save('combine_inception_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz3BREsITlAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "meniscus_model = predictor()\n",
        "meniscus_model.load_weights('combine_inception_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KSBT0duoVaVr",
        "colab": {}
      },
      "source": [
        "# testing the model\n",
        "test_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal', 'test')\n",
        "test_meniscus_coronal_generator = data_generator('meniscus', 'coronal', 'test')\n",
        "test_meniscus_axial_generator = data_generator('meniscus', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = meniscus_sagittal.predict_generator(test_meniscus_sagittal_generator)\n",
        "test_coronal_prediction = meniscus_coronal.predict_generator(test_meniscus_coronal_generator)\n",
        "test_axial_prediction = meniscus_axial.predict_generator(test_meniscus_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('meniscus','test'))\n",
        "\n",
        "meniscus_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLsLjxCM4of-",
        "colab_type": "text"
      },
      "source": [
        "## AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5_ljfNi5Da9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mrnet_model():\n",
        "  alexnet_input = keras.Input(shape=(None,256, 256, 3))\n",
        "  x = layers.Cropping3D(cropping=((0,0),(15,14),(15,14)))(alexnet_input)\n",
        "  x = layers.Conv3D(filters=96, kernel_size=(1,11,11), strides=(1,4,4), padding='valid', activation='relu')(x)\n",
        "  x = layers.MaxPooling3D(pool_size=(1,3,3), strides=(1,2,2), padding='valid')(x)\n",
        "  x = layers.Conv3D(filters=256, kernel_size=(1,5,5), padding='valid', activation='relu')(x)\n",
        "  x = layers.MaxPooling3D(pool_size=(1,3,3), strides=(1,2,2), padding='valid')(x)\n",
        "  x = layers.Conv3D(filters=384, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\n",
        "  alexnet_output = layers.Conv3D(filters=256, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\n",
        "  alexnet = keras.Model(inputs=alexnet_input, outputs=alexnet_output, name='alexnet')\n",
        "  x = layers.AveragePooling3D(strides=(1,7,7))(alexnet_output)\n",
        "  x = layers.GlobalMaxPooling3D()(x)\n",
        "  pooling_output = layers.Dense(1, activation='sigmoid')(x)\n",
        "  mrnet = keras.Model(inputs=alexnet_input, outputs=pooling_output)\n",
        "  mrnet.compile(optimizer='sgd', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "  return mrnet\n",
        "\n",
        "def predictor():\n",
        "  logistic_input = keras.Input(shape=(3,))\n",
        "  logistic_output = layers.Dense(1, activation='sigmoid')(logistic_input)\n",
        "\n",
        "  predictor_model = keras.Model(inputs=logistic_input, outputs=logistic_output)\n",
        "  predictor_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return predictor_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSxchv6aTrGa",
        "colab_type": "text"
      },
      "source": [
        "###Abnormal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEN_3WRbZY_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "abnormal_sagittal = mrnet_model()\n",
        "abnormal_coronal = mrnet_model()\n",
        "abnormal_axial = mrnet_model()\n",
        "\n",
        "training_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "abnormal_sagittal.history = abnormal_sagittal.fit_generator(generator=training_abnormal_sagittal_generator, validation_data=validation_abnormal_sagittal_generator, epochs=30)\n",
        "\n",
        "abnormal_sagittal.save('mrnet_abnormal_sagittal.h5')\n",
        "\n",
        "training_abnormal_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_abnormal_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "abnormal_coronal.history = abnormal_coronal.fit_generator(generator=training_abnormal_coronal_generator, validation_data=validation_abnormal_coronal_generator, epochs=30)\n",
        "\n",
        "abnormal_coronal.save('mrnet_abnormal_coronal.h5')\n",
        "\n",
        "training_abnormal_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_abnormal_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "abnormal_axial.history = abnormal_axial.fit_generator(generator=training_abnormal_axial_generator, validation_data=validation_abnormal_axial_generator, epochs=30)\n",
        "\n",
        "abnormal_axial.save('mrnet_abnormal_axial.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwKux7WBZMDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "abnormal_sagittal = mrnet_model()\n",
        "abnormal_coronal = mrnet_model()\n",
        "abnormal_axial = mrnet_model()\n",
        "\n",
        "abnormal_sagittal.load_weights(\"mrnet_abnormal_sagittal.h5\")\n",
        "abnormal_coronal.load_weights(\"mrnet_abnormal_coronal.h5\")\n",
        "abnormal_axial.load_weights(\"mrnet_abnormal_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHC1hro_R_3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77c878c8-6887-47fc-ac4c-c20d6db90cb1"
      },
      "source": [
        "# combining models\n",
        "training_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "\n",
        "training_abnormal_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_abnormal_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "\n",
        "training_abnormal_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_abnormal_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = abnormal_sagittal.predict_generator(training_abnormal_sagittal_generator)\n",
        "training_coronal_prediction = abnormal_coronal.predict_generator(training_abnormal_coronal_generator)\n",
        "training_axial_prediction = abnormal_axial.predict_generator(training_abnormal_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = abnormal_sagittal.predict_generator(validation_abnormal_sagittal_generator)\n",
        "validation_coronal_prediction = abnormal_coronal.predict_generator(validation_abnormal_coronal_generator)\n",
        "validation_axial_prediction = abnormal_axial.predict_generator(validation_abnormal_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('abnormal','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('abnormal','valid'))\n",
        "\n",
        "abnormal_model = predictor()\n",
        "abnormal_model_hisory = abnormal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "abnormal_model.save('combine_mrnet_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-3ee33b82432c>:11: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.predict, which supports generators.\n",
            "Epoch 1/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4529 - accuracy: 0.7699 - val_loss: 0.4692 - val_accuracy: 0.8053\n",
            "Epoch 2/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.3226 - accuracy: 0.8083 - val_loss: 0.4712 - val_accuracy: 0.8053\n",
            "Epoch 3/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.2635 - accuracy: 0.8456 - val_loss: 0.4810 - val_accuracy: 0.7876\n",
            "Epoch 4/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.2207 - accuracy: 0.9174 - val_loss: 0.4941 - val_accuracy: 0.7965\n",
            "Epoch 5/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9440 - val_loss: 0.5056 - val_accuracy: 0.7876\n",
            "Epoch 6/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1664 - accuracy: 0.9607 - val_loss: 0.5167 - val_accuracy: 0.7876\n",
            "Epoch 7/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1486 - accuracy: 0.9646 - val_loss: 0.5299 - val_accuracy: 0.7965\n",
            "Epoch 8/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1346 - accuracy: 0.9666 - val_loss: 0.5416 - val_accuracy: 0.8053\n",
            "Epoch 9/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1235 - accuracy: 0.9676 - val_loss: 0.5493 - val_accuracy: 0.8053\n",
            "Epoch 10/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1144 - accuracy: 0.9735 - val_loss: 0.5602 - val_accuracy: 0.8053\n",
            "Epoch 11/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1067 - accuracy: 0.9764 - val_loss: 0.5720 - val_accuracy: 0.8053\n",
            "Epoch 12/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1004 - accuracy: 0.9764 - val_loss: 0.5796 - val_accuracy: 0.8053\n",
            "Epoch 13/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0949 - accuracy: 0.9794 - val_loss: 0.5870 - val_accuracy: 0.7965\n",
            "Epoch 14/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0901 - accuracy: 0.9813 - val_loss: 0.5956 - val_accuracy: 0.7965\n",
            "Epoch 15/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0859 - accuracy: 0.9794 - val_loss: 0.5999 - val_accuracy: 0.7965\n",
            "Epoch 16/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0823 - accuracy: 0.9823 - val_loss: 0.6104 - val_accuracy: 0.7965\n",
            "Epoch 17/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0790 - accuracy: 0.9833 - val_loss: 0.6175 - val_accuracy: 0.7965\n",
            "Epoch 18/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0761 - accuracy: 0.9833 - val_loss: 0.6246 - val_accuracy: 0.7965\n",
            "Epoch 19/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0735 - accuracy: 0.9833 - val_loss: 0.6285 - val_accuracy: 0.7965\n",
            "Epoch 20/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0711 - accuracy: 0.9833 - val_loss: 0.6349 - val_accuracy: 0.7965\n",
            "Epoch 21/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0690 - accuracy: 0.9833 - val_loss: 0.6416 - val_accuracy: 0.7965\n",
            "Epoch 22/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0670 - accuracy: 0.9833 - val_loss: 0.6458 - val_accuracy: 0.7965\n",
            "Epoch 23/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0652 - accuracy: 0.9843 - val_loss: 0.6517 - val_accuracy: 0.7965\n",
            "Epoch 24/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0635 - accuracy: 0.9843 - val_loss: 0.6576 - val_accuracy: 0.7876\n",
            "Epoch 25/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0620 - accuracy: 0.9862 - val_loss: 0.6619 - val_accuracy: 0.7876\n",
            "Epoch 26/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0605 - accuracy: 0.9862 - val_loss: 0.6691 - val_accuracy: 0.7876\n",
            "Epoch 27/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0592 - accuracy: 0.9862 - val_loss: 0.6743 - val_accuracy: 0.7876\n",
            "Epoch 28/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0580 - accuracy: 0.9862 - val_loss: 0.6774 - val_accuracy: 0.7876\n",
            "Epoch 29/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0568 - accuracy: 0.9862 - val_loss: 0.6829 - val_accuracy: 0.7876\n",
            "Epoch 30/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0557 - accuracy: 0.9853 - val_loss: 0.6879 - val_accuracy: 0.7876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN8T3apdMbQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "abnormal_model = predictor()\n",
        "abnormal_model.load_weights('combine_mrnet_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNvUscSw4U5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0d90ef78-249f-4f17-d4ba-10cf23ad5a92"
      },
      "source": [
        "# testing the model\n",
        "test_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal', 'test')\n",
        "test_abnormal_coronal_generator = data_generator('abnormal', 'coronal', 'test')\n",
        "test_abnormal_axial_generator = data_generator('abnormal', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = abnormal_sagittal.predict_generator(test_abnormal_sagittal_generator)\n",
        "test_coronal_prediction = abnormal_coronal.predict_generator(test_abnormal_coronal_generator)\n",
        "test_axial_prediction = abnormal_axial.predict_generator(test_abnormal_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('abnormal','test'))\n",
        "\n",
        "abnormal_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 2ms/step - loss: 0.7697 - accuracy: 0.7917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7696942687034607, 0.7916666865348816]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5L4jMA1TzzZ",
        "colab_type": "text"
      },
      "source": [
        "###ACL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrMU4bYPZlnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "acl_sagittal = mrnet_model()\n",
        "acl_coronal = mrnet_model()\n",
        "acl_axial = mrnet_model()\n",
        "\n",
        "training_acl_sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_acl_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "acl_sagittal.history = acl_sagittal.fit_generator(generator=training_acl_sagittal_generator, validation_data=validation_acl_sagittal_generator, epochs=30)\n",
        "\n",
        "acl_sagittal.save('mrnet_acl_sagittal.h5')\n",
        "\n",
        "training_acl_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_acl_axial_generator = data_generator('acl', 'axial','valid')\n",
        "acl_axial.history = acl_axial.fit_generator(generator=training_acl_axial_generator, validation_data=validation_acl_axial_generator, epochs=30)\n",
        "\n",
        "acl_axial.save('mrnet_acl_axial.h5')\n",
        "\n",
        "training_acl_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_acl_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "acl_coronal.history = acl_coronal.fit_generator(generator=training_acl_coronal_generator, validation_data=validation_acl_coronal_generator, epochs=30)\n",
        "\n",
        "acl_coronal.save('mrnet_acl_coronal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asrOOBwgamJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "acl_sagittal = mrnet_model()\n",
        "acl_coronal = mrnet_model()\n",
        "acl_axial = mrnet_model()\n",
        "\n",
        "acl_sagittal.load_weights(\"mrnet_acl_sagittal.h5\")\n",
        "acl_coronal.load_weights(\"mrnet_acl_coronal.h5\")\n",
        "acl_axial.load_weights(\"mrnet_acl_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GB02uGgsai2R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a42adcb6-460e-4d60-ca3c-328337d143f6"
      },
      "source": [
        "# combining models\n",
        "training_acl_sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_acl_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "\n",
        "training_acl_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_acl_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "\n",
        "training_acl_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_acl_axial_generator = data_generator('acl', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = acl_sagittal.predict_generator(training_acl_sagittal_generator)\n",
        "training_coronal_prediction = acl_coronal.predict_generator(training_acl_coronal_generator)\n",
        "training_axial_prediction = acl_axial.predict_generator(training_acl_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = acl_sagittal.predict_generator(validation_acl_sagittal_generator)\n",
        "validation_coronal_prediction = acl_coronal.predict_generator(validation_acl_coronal_generator)\n",
        "validation_axial_prediction = acl_axial.predict_generator(validation_acl_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('acl','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('acl','valid'))\n",
        "\n",
        "acl_model = predictor()\n",
        "acl_model_hisory = acl_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "acl_model.save('combine_mrnet_acl.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.3786 - accuracy: 0.9725 - val_loss: 0.4302 - val_accuracy: 0.8230\n",
            "Epoch 2/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9833 - val_loss: 0.4228 - val_accuracy: 0.8142\n",
            "Epoch 3/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1236 - accuracy: 0.9931 - val_loss: 0.4317 - val_accuracy: 0.8142\n",
            "Epoch 4/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0917 - accuracy: 0.9980 - val_loss: 0.4428 - val_accuracy: 0.8142\n",
            "Epoch 5/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0729 - accuracy: 0.9990 - val_loss: 0.4543 - val_accuracy: 0.8142\n",
            "Epoch 6/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0605 - accuracy: 1.0000 - val_loss: 0.4652 - val_accuracy: 0.8142\n",
            "Epoch 7/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0518 - accuracy: 1.0000 - val_loss: 0.4753 - val_accuracy: 0.8142\n",
            "Epoch 8/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0453 - accuracy: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.8053\n",
            "Epoch 9/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 0.4934 - val_accuracy: 0.8053\n",
            "Epoch 10/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.5016 - val_accuracy: 0.8053\n",
            "Epoch 11/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.5092 - val_accuracy: 0.8053\n",
            "Epoch 12/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.5163 - val_accuracy: 0.8053\n",
            "Epoch 13/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.5230 - val_accuracy: 0.8053\n",
            "Epoch 14/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.5293 - val_accuracy: 0.8053\n",
            "Epoch 15/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.5353 - val_accuracy: 0.8053\n",
            "Epoch 16/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.5410 - val_accuracy: 0.8053\n",
            "Epoch 17/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.5465 - val_accuracy: 0.8053\n",
            "Epoch 18/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.5516 - val_accuracy: 0.8053\n",
            "Epoch 19/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.5566 - val_accuracy: 0.8053\n",
            "Epoch 20/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.5613 - val_accuracy: 0.8053\n",
            "Epoch 21/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.5658 - val_accuracy: 0.8053\n",
            "Epoch 22/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.5702 - val_accuracy: 0.8053\n",
            "Epoch 23/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.5744 - val_accuracy: 0.8053\n",
            "Epoch 24/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.5785 - val_accuracy: 0.8053\n",
            "Epoch 25/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.5824 - val_accuracy: 0.8053\n",
            "Epoch 26/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.5862 - val_accuracy: 0.8053\n",
            "Epoch 27/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.5899 - val_accuracy: 0.8053\n",
            "Epoch 28/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.5934 - val_accuracy: 0.8053\n",
            "Epoch 29/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.5969 - val_accuracy: 0.8053\n",
            "Epoch 30/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 0.8053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ApeSCegwbX4y",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "acl_model = predictor()\n",
        "acl_model.load_weights('combine_mrnet_acl.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NIwIvGD5bl-w",
        "colab": {}
      },
      "source": [
        "# testing the model\n",
        "test_acl_sagittal_generator = data_generator('acl', 'sagittal', 'test')\n",
        "test_acl_coronal_generator = data_generator('acl', 'coronal', 'test')\n",
        "test_acl_axial_generator = data_generator('acl', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = acl_sagittal.predict_generator(test_acl_sagittal_generator)\n",
        "test_coronal_prediction = acl_coronal.predict_generator(test_acl_coronal_generator)\n",
        "test_axial_prediction = acl_axial.predict_generator(test_acl_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('acl','test'))\n",
        "\n",
        "acl_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TklZQfVT1vW",
        "colab_type": "text"
      },
      "source": [
        "###Meniscus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JXKp435Zl0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "meniscus_sagittal = mrnet_model()\n",
        "meniscus_coronal = mrnet_model()\n",
        "meniscus_axial = mrnet_model()\n",
        "\n",
        "training_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "meniscus_axial.history = meniscus_sagittal.fit_generator(generator=training_meniscus_sagittal_generator, validation_data=validation_meniscus_sagittal_generator, epochs=30)\n",
        "\n",
        "meniscus_sagittal.save('mrnet_meniscus_sagittal.h5')\n",
        "\n",
        "training_meniscus_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_meniscus_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "meniscus_axial.history = meniscus_axial.fit_generator(generator=training_meniscus_axial_generator, validation_data=validation_meniscus_axial_generator, epochs=30)\n",
        "\n",
        "meniscus_axial.save('mrnet_meniscus_axial.h5')\n",
        "\n",
        "training_meniscus_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_meniscus_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "meniscus_coronal.history = meniscus_coronal.fit_generator(generator=training_meniscus_coronal_generator, validation_data=validation_meniscus_coronal_generator, epochs=30)\n",
        "\n",
        "meniscus_coronal.save('mrnet_meniscus_coronal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXAWARiqazjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "meniscus_sagittal = mrnet_model()\n",
        "meniscus_coronal = mrnet_model()\n",
        "meniscus_axial = mrnet_model()\n",
        "\n",
        "meniscus_sagittal.load_weights(\"mrnet_meniscus_sagittal.h5\")\n",
        "meniscus_coronal.load_weights(\"mrnet_meniscus_coronal.h5\")\n",
        "meniscus_axial.load_weights(\"mrnet_meniscus_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yddHXmp1b4rp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1ff19e2-04ee-4726-d71b-b90b93b44527"
      },
      "source": [
        "# combining models\n",
        "training_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "\n",
        "training_meniscus_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_meniscus_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "\n",
        "training_meniscus_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_meniscus_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = meniscus_sagittal.predict_generator(training_meniscus_sagittal_generator)\n",
        "training_coronal_prediction = meniscus_coronal.predict_generator(training_meniscus_coronal_generator)\n",
        "training_axial_prediction = meniscus_axial.predict_generator(training_meniscus_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = meniscus_sagittal.predict_generator(validation_meniscus_sagittal_generator)\n",
        "validation_coronal_prediction = meniscus_coronal.predict_generator(validation_meniscus_coronal_generator)\n",
        "validation_axial_prediction = meniscus_axial.predict_generator(validation_meniscus_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('meniscus','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('meniscus','valid'))\n",
        "\n",
        "meniscus_model = predictor()\n",
        "meniscus_model_hisory = meniscus_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "meniscus_model.save('combine_mrnet_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1017/1017 [==============================] - 3s 3ms/step - loss: 0.4783 - accuracy: 0.9154 - val_loss: 0.5866 - val_accuracy: 0.7257\n",
            "Epoch 2/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.2396 - accuracy: 0.9961 - val_loss: 0.5885 - val_accuracy: 0.7257\n",
            "Epoch 3/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1549 - accuracy: 0.9990 - val_loss: 0.6102 - val_accuracy: 0.7168\n",
            "Epoch 4/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.1137 - accuracy: 1.0000 - val_loss: 0.6340 - val_accuracy: 0.7168\n",
            "Epoch 5/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0896 - accuracy: 1.0000 - val_loss: 0.6570 - val_accuracy: 0.7168\n",
            "Epoch 6/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0739 - accuracy: 1.0000 - val_loss: 0.6791 - val_accuracy: 0.7168\n",
            "Epoch 7/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0628 - accuracy: 1.0000 - val_loss: 0.6993 - val_accuracy: 0.7080\n",
            "Epoch 8/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.7177 - val_accuracy: 0.7080\n",
            "Epoch 9/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 0.7348 - val_accuracy: 0.7080\n",
            "Epoch 10/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 0.7507 - val_accuracy: 0.7080\n",
            "Epoch 11/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.7655 - val_accuracy: 0.7080\n",
            "Epoch 12/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.7795 - val_accuracy: 0.7080\n",
            "Epoch 13/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.7926 - val_accuracy: 0.7080\n",
            "Epoch 14/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.8048 - val_accuracy: 0.7080\n",
            "Epoch 15/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.8166 - val_accuracy: 0.7080\n",
            "Epoch 16/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.8277 - val_accuracy: 0.7080\n",
            "Epoch 17/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.8382 - val_accuracy: 0.7080\n",
            "Epoch 18/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.8483 - val_accuracy: 0.7080\n",
            "Epoch 19/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.8579 - val_accuracy: 0.7080\n",
            "Epoch 20/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.8670 - val_accuracy: 0.7080\n",
            "Epoch 21/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.8759 - val_accuracy: 0.7080\n",
            "Epoch 22/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.8844 - val_accuracy: 0.7080\n",
            "Epoch 23/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.8926 - val_accuracy: 0.7080\n",
            "Epoch 24/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.9006 - val_accuracy: 0.7080\n",
            "Epoch 25/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.9082 - val_accuracy: 0.7080\n",
            "Epoch 26/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.9156 - val_accuracy: 0.7080\n",
            "Epoch 27/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.9228 - val_accuracy: 0.7080\n",
            "Epoch 28/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.9297 - val_accuracy: 0.7080\n",
            "Epoch 29/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.9364 - val_accuracy: 0.7080\n",
            "Epoch 30/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.9429 - val_accuracy: 0.7080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe5y4iGZcwPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "meniscus_model = predictor()\n",
        "meniscus_model.load_weights('combine_mrnet_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Baw0gcsGc3bo",
        "colab": {}
      },
      "source": [
        "# testing the model\n",
        "test_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal', 'test')\n",
        "test_meniscus_coronal_generator = data_generator('meniscus', 'coronal', 'test')\n",
        "test_meniscus_axial_generator = data_generator('meniscus', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = meniscus_sagittal.predict_generator(test_meniscus_sagittal_generator)\n",
        "test_coronal_prediction = meniscusl_coronal.predict_generator(test_meniscus_coronal_generator)\n",
        "test_axial_prediction = meniscus_axial.predict_generator(test_meniscus_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('meniscus','test'))\n",
        "\n",
        "meniscus_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxq-K7E43SIC",
        "colab_type": "text"
      },
      "source": [
        "#Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_QsKl7UXKkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "def transfer():\n",
        "  inputs = keras.Input(shape=(None,256,256,3))\n",
        "\n",
        "  conv_base = InceptionV3(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(256, 256, 3))(inputs[0])\n",
        "\n",
        "  avg_layer = layers.GlobalAveragePooling2D()(conv_base)\n",
        "  avg_layer = tf.expand_dims(avg_layer,0)\n",
        "  max_layer = layers.GlobalMaxPooling1D()(avg_layer)\n",
        "  flat = layers.Flatten()(max_layer)\n",
        "  dense = layers.Dense(512,activation='relu')(flat)\n",
        "  out = layers.Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "  model = models.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "  model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "t = transfer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5i4WCT5ePPnK"
      },
      "source": [
        "###Abnormal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pWkes7t7f6Vg",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "abnormal_sagittal = transfer()\n",
        "abnormal_coronal = transfer()\n",
        "abnormal_axial = transfer()\n",
        "\n",
        "training_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "abnormal_sagittal.history = abnormal_sagittal.fit_generator(generator=training_abnormal_sagittal_generator, validation_data=validation_abnormal_sagittal_generator, epochs=1)\n",
        "\n",
        "abnormal_sagittal.save('transfer_abnormal_sagittal.h5')\n",
        "\n",
        "training_abnormal_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_abnormal_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "abnormal_coronal.history = abnormal_coronal.fit_generator(generator=training_abnormal_coronal_generator, validation_data=validation_abnormal_coronal_generator, epochs=1)\n",
        "\n",
        "abnormal_coronal.save('transfer_abnormal_coronal.h5')\n",
        "\n",
        "training_abnormal_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_abnormal_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "abnormal_axial.history = abnormal_axial.fit_generator(generator=training_abnormal_axial_generator, validation_data=validation_abnormal_axial_generator, epochs=1)\n",
        "\n",
        "abnormal_axial.save('transfer_abnormal_axial.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bn-wErSsgUYE",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "abnormal_sagittal = transfer()\n",
        "abnormal_coronal = transfer()\n",
        "abnormal_axial = transfer()\n",
        "\n",
        "abnormal_sagittal.load_weights(\"transfer_abnormal_sagittal.h5\")\n",
        "abnormal_coronal.load_weights(\"transfer_abnormal_coronal.h5\")\n",
        "abnormal_axial.load_weights(\"transfer_abnormal_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k01t2VCTgohN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0589266f-2bcd-46a1-a867-c6fad40c9e04"
      },
      "source": [
        "# combining models\n",
        "training_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "\n",
        "training_abnormal_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_abnormal_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "\n",
        "training_abnormal_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_abnormal_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = abnormal_sagittal.predict_generator(training_abnormal_sagittal_generator)\n",
        "training_coronal_prediction = abnormal_coronal.predict_generator(training_abnormal_coronal_generator)\n",
        "training_axial_prediction = abnormal_axial.predict_generator(training_abnormal_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = abnormal_sagittal.predict_generator(validation_abnormal_sagittal_generator)\n",
        "validation_coronal_prediction = abnormal_coronal.predict_generator(validation_abnormal_coronal_generator)\n",
        "validation_axial_prediction = abnormal_axial.predict_generator(validation_abnormal_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('abnormal','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('abnormal','valid'))\n",
        "\n",
        "abnormal_model = predictor()\n",
        "abnormal_model.hisory = abnormal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "abnormal_model.save('combine_transfer_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.5218 - accuracy: 0.7847 - val_loss: 0.4970 - val_accuracy: 0.8053\n",
            "Epoch 2/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4952 - accuracy: 0.8083 - val_loss: 0.4973 - val_accuracy: 0.8053\n",
            "Epoch 3/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4944 - accuracy: 0.8083 - val_loss: 0.4968 - val_accuracy: 0.8053\n",
            "Epoch 4/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4931 - accuracy: 0.8083 - val_loss: 0.4949 - val_accuracy: 0.8053\n",
            "Epoch 5/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4921 - accuracy: 0.8083 - val_loss: 0.4950 - val_accuracy: 0.8053\n",
            "Epoch 6/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4916 - accuracy: 0.8083 - val_loss: 0.4938 - val_accuracy: 0.8053\n",
            "Epoch 7/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4900 - accuracy: 0.8083 - val_loss: 0.4941 - val_accuracy: 0.8053\n",
            "Epoch 8/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4895 - accuracy: 0.8083 - val_loss: 0.4940 - val_accuracy: 0.8053\n",
            "Epoch 9/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4885 - accuracy: 0.8083 - val_loss: 0.4943 - val_accuracy: 0.8053\n",
            "Epoch 10/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4879 - accuracy: 0.8083 - val_loss: 0.4924 - val_accuracy: 0.8053\n",
            "Epoch 11/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4863 - accuracy: 0.8083 - val_loss: 0.4955 - val_accuracy: 0.8053\n",
            "Epoch 12/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4870 - accuracy: 0.8083 - val_loss: 0.4920 - val_accuracy: 0.8053\n",
            "Epoch 13/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4859 - accuracy: 0.8083 - val_loss: 0.4917 - val_accuracy: 0.8053\n",
            "Epoch 14/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4855 - accuracy: 0.8083 - val_loss: 0.4916 - val_accuracy: 0.8053\n",
            "Epoch 15/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4841 - accuracy: 0.8083 - val_loss: 0.4916 - val_accuracy: 0.8053\n",
            "Epoch 16/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4839 - accuracy: 0.8083 - val_loss: 0.4937 - val_accuracy: 0.8053\n",
            "Epoch 17/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4841 - accuracy: 0.8083 - val_loss: 0.4914 - val_accuracy: 0.8053\n",
            "Epoch 18/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4834 - accuracy: 0.8083 - val_loss: 0.4914 - val_accuracy: 0.8053\n",
            "Epoch 19/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4830 - accuracy: 0.8083 - val_loss: 0.4918 - val_accuracy: 0.8053\n",
            "Epoch 20/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4825 - accuracy: 0.8083 - val_loss: 0.4924 - val_accuracy: 0.8053\n",
            "Epoch 21/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4825 - accuracy: 0.8083 - val_loss: 0.4914 - val_accuracy: 0.8053\n",
            "Epoch 22/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4814 - accuracy: 0.8083 - val_loss: 0.4914 - val_accuracy: 0.8053\n",
            "Epoch 23/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4810 - accuracy: 0.8083 - val_loss: 0.4927 - val_accuracy: 0.8053\n",
            "Epoch 24/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4796 - accuracy: 0.8083 - val_loss: 0.4918 - val_accuracy: 0.8053\n",
            "Epoch 25/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4801 - accuracy: 0.8083 - val_loss: 0.4917 - val_accuracy: 0.8053\n",
            "Epoch 26/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4805 - accuracy: 0.8083 - val_loss: 0.4919 - val_accuracy: 0.8053\n",
            "Epoch 27/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4808 - accuracy: 0.8083 - val_loss: 0.4922 - val_accuracy: 0.8053\n",
            "Epoch 28/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4788 - accuracy: 0.8083 - val_loss: 0.4924 - val_accuracy: 0.8053\n",
            "Epoch 29/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4798 - accuracy: 0.8083 - val_loss: 0.4930 - val_accuracy: 0.8053\n",
            "Epoch 30/30\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.4801 - accuracy: 0.8083 - val_loss: 0.4924 - val_accuracy: 0.8053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR3rWUdAg_ZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "abnormal_model = predictor()\n",
        "abnormal_model.load_weights('combine_transfer_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mqECPs6niJsH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "eebfc690-408e-40d5-b7f1-5d1e5c98bb92"
      },
      "source": [
        "# testing the model\n",
        "test_abnormal_sagittal_generator = data_generator('abnormal', 'sagittal', 'test')\n",
        "test_abnormal_coronal_generator = data_generator('abnormal', 'coronal', 'test')\n",
        "test_abnormal_axial_generator = data_generator('abnormal', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = abnormal_sagittal.predict_generator(test_abnormal_sagittal_generator)\n",
        "test_coronal_prediction = abnormal_coronal.predict_generator(test_abnormal_coronal_generator)\n",
        "test_axial_prediction = abnormal_axial.predict_generator(test_abnormal_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('abnormal','test'))\n",
        "\n",
        "abnormal_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5050693154335022, 0.7916666865348816]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KSCLmbDQPPwZ"
      },
      "source": [
        "###ACL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4bD-j0SRiYhb",
        "colab": {}
      },
      "source": [
        "# training models\n",
        "acl_sagittal = transfer()\n",
        "acl_coronal = transfer()\n",
        "acl_axial = transfer()\n",
        "\n",
        "training_acl_sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_acl_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "acl_sagittal.history = acl_sagittal.fit_generator(generator=training_acl_sagittal_generator, validation_data=validation_acl_sagittal_generator, epochs=1)\n",
        "\n",
        "acl_sagittal.save('transfer_acl_sagittal.h5')\n",
        "\n",
        "training_acl_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_acl_axial_generator = data_generator('acl', 'axial','valid')\n",
        "acl_axial.history = acl_axial.fit_generator(generator=training_acl_axial_generator, validation_data=validation_acl_axial_generator, epochs=1)\n",
        "\n",
        "acl_axial.save('transfer_acl_axial.h5')\n",
        "\n",
        "training_acl_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_acl_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "acl_coronal.history = acl_coronal.fit_generator(generator=training_acl_coronal_generator, validation_data=validation_acl_coronal_generator, epochs=1)\n",
        "\n",
        "acl_coronal.save('transfer_acl_coronal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fWFudBowiYhh",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "acl_sagittal = transfer()\n",
        "acl_coronal = transfer()\n",
        "acl_axial = transfer()\n",
        "\n",
        "acl_sagittal.load_weights(\"transfer_acl_sagittal.h5\")\n",
        "acl_coronal.load_weights(\"transfer_acl_coronal.h5\")\n",
        "acl_axial.load_weights(\"transfer_acl_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q49569HEiYhm",
        "colab": {}
      },
      "source": [
        "# combining models\n",
        "training_acl_sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_acl_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "\n",
        "training_acl_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_acl_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "\n",
        "training_acl_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_acl_axial_generator = data_generator('acl', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = acl_sagittal.predict_generator(training_acl_sagittal_generator)\n",
        "training_coronal_prediction = acl_coronal.predict_generator(training_acl_coronal_generator)\n",
        "training_axial_prediction = acl_axial.predict_generator(training_acl_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = acl_sagittal.predict_generator(validation_acl_sagittal_generator)\n",
        "validation_coronal_prediction = acl_coronal.predict_generator(validation_acl_coronal_generator)\n",
        "validation_axial_prediction = acl_axial.predict_generator(validation_acl_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('acl','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('acl','valid'))\n",
        "\n",
        "acl_model = predictor()\n",
        "acl_model.hisory = acl_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "acl_model.save('combine_transfer_acl.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uae-TBe-iYhq",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "acl_model = predictor()\n",
        "acl_model.load_weights('combine_transfer_acl.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nNLppEMtiYht",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a31cd859-5dfe-491f-cbb9-bd745d8a2ac6"
      },
      "source": [
        "# testing the model\n",
        "test_acl_sagittal_generator = data_generator('acl', 'sagittal', 'test')\n",
        "test_acl_coronal_generator = data_generator('acl', 'coronal', 'test')\n",
        "test_acl_axial_generator = data_generator('acl', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = acl_sagittal.predict_generator(test_acl_sagittal_generator)\n",
        "test_coronal_prediction = acl_coronal.predict_generator(test_acl_coronal_generator)\n",
        "test_axial_prediction = acl_axial.predict_generator(test_acl_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('acl','test'))\n",
        "\n",
        "acl_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 2ms/step - loss: 0.8596 - accuracy: 0.5500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8596245050430298, 0.550000011920929]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kSHJGf2rPP0Y"
      },
      "source": [
        "###Meniscus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WhRVcFxXjHhI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2ac84eb1-0b08-4730-9a01-20743efcf20b"
      },
      "source": [
        "# training models\n",
        "meniscus_sagittal = transfer()\n",
        "meniscus_coronal = transfer()\n",
        "meniscus_axial = transfer()\n",
        "\n",
        "training_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "meniscus_axial.history = meniscus_sagittal.fit_generator(generator=training_meniscus_sagittal_generator, validation_data=validation_meniscus_sagittal_generator, epochs=1)\n",
        "\n",
        "meniscus_sagittal.save('transfer_meniscus_sagittal.h5')\n",
        "\n",
        "training_meniscus_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_meniscus_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "meniscus_axial.history = meniscus_axial.fit_generator(generator=training_meniscus_axial_generator, validation_data=validation_meniscus_axial_generator, epochs=1)\n",
        "\n",
        "meniscus_axial.save('transfer_meniscus_axial.h5')\n",
        "\n",
        "training_meniscus_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_meniscus_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "meniscus_coronal.history = meniscus_coronal.fit_generator(generator=training_meniscus_coronal_generator, validation_data=validation_meniscus_coronal_generator, epochs=1)\n",
        "\n",
        "meniscus_coronal.save('transfer_meniscus_coronal.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1017/1017 [==============================] - 271s 266ms/step - loss: 0.6783 - accuracy: 0.6273 - val_loss: 0.6778 - val_accuracy: 0.6637\n",
            "1017/1017 [==============================] - 214s 210ms/step - loss: 0.6734 - accuracy: 0.6372 - val_loss: 0.6344 - val_accuracy: 0.6460\n",
            "1017/1017 [==============================] - 304s 299ms/step - loss: 0.6852 - accuracy: 0.6313 - val_loss: 0.6659 - val_accuracy: 0.6283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AfyQtGymjHhQ",
        "colab": {}
      },
      "source": [
        "# loading trained models\n",
        "meniscus_sagittal = transfer()\n",
        "meniscus_coronal = transfer()\n",
        "meniscus_axial = transfer()\n",
        "\n",
        "meniscus_sagittal.load_weights(\"transfer_meniscus_sagittal.h5\")\n",
        "meniscus_coronal.load_weights(\"transfer_meniscus_coronal.h5\")\n",
        "meniscus_axial.load_weights(\"transfer_meniscus_axial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "txym2s5_jHhV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21fd4ee9-5e1a-492f-9cd8-5e96f9382f5e"
      },
      "source": [
        "# combining models\n",
        "training_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "\n",
        "training_meniscus_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_meniscus_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "\n",
        "training_meniscus_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_meniscus_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "\n",
        "training_sagittal_prediction = meniscus_sagittal.predict_generator(training_meniscus_sagittal_generator)\n",
        "training_coronal_prediction = meniscus_coronal.predict_generator(training_meniscus_coronal_generator)\n",
        "training_axial_prediction = meniscus_axial.predict_generator(training_meniscus_axial_generator)\n",
        "\n",
        "validation_sagittal_prediction = meniscus_sagittal.predict_generator(validation_meniscus_sagittal_generator)\n",
        "validation_coronal_prediction = meniscus_coronal.predict_generator(validation_meniscus_coronal_generator)\n",
        "validation_axial_prediction = meniscus_axial.predict_generator(validation_meniscus_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('meniscus','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('meniscus','valid'))\n",
        "\n",
        "meniscus_model = predictor()\n",
        "meniscus_model.hisory = meniscus_model.fit(training_input, training_output,epochs=100,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "meniscus_model.save('combine_transfer_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6698 - accuracy: 0.5998 - val_loss: 0.6403 - val_accuracy: 0.6637\n",
            "Epoch 2/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6489 - accuracy: 0.6470 - val_loss: 0.6401 - val_accuracy: 0.6637\n",
            "Epoch 3/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6479 - accuracy: 0.6470 - val_loss: 0.6371 - val_accuracy: 0.6637\n",
            "Epoch 4/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6458 - accuracy: 0.6470 - val_loss: 0.6350 - val_accuracy: 0.6637\n",
            "Epoch 5/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6445 - accuracy: 0.6470 - val_loss: 0.6338 - val_accuracy: 0.6637\n",
            "Epoch 6/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6442 - accuracy: 0.6470 - val_loss: 0.6328 - val_accuracy: 0.6637\n",
            "Epoch 7/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6427 - accuracy: 0.6470 - val_loss: 0.6324 - val_accuracy: 0.6637\n",
            "Epoch 8/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6414 - accuracy: 0.6470 - val_loss: 0.6315 - val_accuracy: 0.6637\n",
            "Epoch 9/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6403 - accuracy: 0.6470 - val_loss: 0.6298 - val_accuracy: 0.6637\n",
            "Epoch 10/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6388 - accuracy: 0.6470 - val_loss: 0.6318 - val_accuracy: 0.6637\n",
            "Epoch 11/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6381 - accuracy: 0.6470 - val_loss: 0.6280 - val_accuracy: 0.6637\n",
            "Epoch 12/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6368 - accuracy: 0.6470 - val_loss: 0.6273 - val_accuracy: 0.6637\n",
            "Epoch 13/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6362 - accuracy: 0.6470 - val_loss: 0.6283 - val_accuracy: 0.6637\n",
            "Epoch 14/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6352 - accuracy: 0.6470 - val_loss: 0.6255 - val_accuracy: 0.6637\n",
            "Epoch 15/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6337 - accuracy: 0.6470 - val_loss: 0.6248 - val_accuracy: 0.6637\n",
            "Epoch 16/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6339 - accuracy: 0.6470 - val_loss: 0.6244 - val_accuracy: 0.6637\n",
            "Epoch 17/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6327 - accuracy: 0.6470 - val_loss: 0.6236 - val_accuracy: 0.6637\n",
            "Epoch 18/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6317 - accuracy: 0.6470 - val_loss: 0.6232 - val_accuracy: 0.6637\n",
            "Epoch 19/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6307 - accuracy: 0.6470 - val_loss: 0.6218 - val_accuracy: 0.6637\n",
            "Epoch 20/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6302 - accuracy: 0.6470 - val_loss: 0.6217 - val_accuracy: 0.6637\n",
            "Epoch 21/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6291 - accuracy: 0.6470 - val_loss: 0.6203 - val_accuracy: 0.6637\n",
            "Epoch 22/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6287 - accuracy: 0.6470 - val_loss: 0.6197 - val_accuracy: 0.6637\n",
            "Epoch 23/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6276 - accuracy: 0.6470 - val_loss: 0.6203 - val_accuracy: 0.6637\n",
            "Epoch 24/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6272 - accuracy: 0.6470 - val_loss: 0.6195 - val_accuracy: 0.6637\n",
            "Epoch 25/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6265 - accuracy: 0.6470 - val_loss: 0.6184 - val_accuracy: 0.6637\n",
            "Epoch 26/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6243 - accuracy: 0.6470 - val_loss: 0.6200 - val_accuracy: 0.6637\n",
            "Epoch 27/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6250 - accuracy: 0.6470 - val_loss: 0.6170 - val_accuracy: 0.6637\n",
            "Epoch 28/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6244 - accuracy: 0.6470 - val_loss: 0.6166 - val_accuracy: 0.6637\n",
            "Epoch 29/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6239 - accuracy: 0.6470 - val_loss: 0.6161 - val_accuracy: 0.6637\n",
            "Epoch 30/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6231 - accuracy: 0.6470 - val_loss: 0.6165 - val_accuracy: 0.6637\n",
            "Epoch 31/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6224 - accuracy: 0.6470 - val_loss: 0.6168 - val_accuracy: 0.6637\n",
            "Epoch 32/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6221 - accuracy: 0.6470 - val_loss: 0.6155 - val_accuracy: 0.6637\n",
            "Epoch 33/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6213 - accuracy: 0.6470 - val_loss: 0.6153 - val_accuracy: 0.6637\n",
            "Epoch 34/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6209 - accuracy: 0.6470 - val_loss: 0.6133 - val_accuracy: 0.6637\n",
            "Epoch 35/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6200 - accuracy: 0.6470 - val_loss: 0.6129 - val_accuracy: 0.6637\n",
            "Epoch 36/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6199 - accuracy: 0.6470 - val_loss: 0.6140 - val_accuracy: 0.6637\n",
            "Epoch 37/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6193 - accuracy: 0.6470 - val_loss: 0.6123 - val_accuracy: 0.6637\n",
            "Epoch 38/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6184 - accuracy: 0.6470 - val_loss: 0.6131 - val_accuracy: 0.6637\n",
            "Epoch 39/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6184 - accuracy: 0.6470 - val_loss: 0.6122 - val_accuracy: 0.6637\n",
            "Epoch 40/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6180 - accuracy: 0.6470 - val_loss: 0.6115 - val_accuracy: 0.6637\n",
            "Epoch 41/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6173 - accuracy: 0.6470 - val_loss: 0.6122 - val_accuracy: 0.6637\n",
            "Epoch 42/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6171 - accuracy: 0.6470 - val_loss: 0.6109 - val_accuracy: 0.6637\n",
            "Epoch 43/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6154 - accuracy: 0.6480 - val_loss: 0.6101 - val_accuracy: 0.6637\n",
            "Epoch 44/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6159 - accuracy: 0.6470 - val_loss: 0.6114 - val_accuracy: 0.6637\n",
            "Epoch 45/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6158 - accuracy: 0.6470 - val_loss: 0.6100 - val_accuracy: 0.6637\n",
            "Epoch 46/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6149 - accuracy: 0.6470 - val_loss: 0.6088 - val_accuracy: 0.6637\n",
            "Epoch 47/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6150 - accuracy: 0.6470 - val_loss: 0.6085 - val_accuracy: 0.6637\n",
            "Epoch 48/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6147 - accuracy: 0.6470 - val_loss: 0.6088 - val_accuracy: 0.6637\n",
            "Epoch 49/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6142 - accuracy: 0.6470 - val_loss: 0.6097 - val_accuracy: 0.6637\n",
            "Epoch 50/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6139 - accuracy: 0.6490 - val_loss: 0.6080 - val_accuracy: 0.6637\n",
            "Epoch 51/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6134 - accuracy: 0.6470 - val_loss: 0.6076 - val_accuracy: 0.6637\n",
            "Epoch 52/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6132 - accuracy: 0.6470 - val_loss: 0.6071 - val_accuracy: 0.6637\n",
            "Epoch 53/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6130 - accuracy: 0.6470 - val_loss: 0.6068 - val_accuracy: 0.6637\n",
            "Epoch 54/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6122 - accuracy: 0.6470 - val_loss: 0.6090 - val_accuracy: 0.6637\n",
            "Epoch 55/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6118 - accuracy: 0.6549 - val_loss: 0.6065 - val_accuracy: 0.6637\n",
            "Epoch 56/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6121 - accuracy: 0.6480 - val_loss: 0.6065 - val_accuracy: 0.6637\n",
            "Epoch 57/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6115 - accuracy: 0.6480 - val_loss: 0.6076 - val_accuracy: 0.6637\n",
            "Epoch 58/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6107 - accuracy: 0.6470 - val_loss: 0.6092 - val_accuracy: 0.6726\n",
            "Epoch 59/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6111 - accuracy: 0.6490 - val_loss: 0.6055 - val_accuracy: 0.6637\n",
            "Epoch 60/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6110 - accuracy: 0.6480 - val_loss: 0.6056 - val_accuracy: 0.6637\n",
            "Epoch 61/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6106 - accuracy: 0.6480 - val_loss: 0.6051 - val_accuracy: 0.6637\n",
            "Epoch 62/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6103 - accuracy: 0.6509 - val_loss: 0.6051 - val_accuracy: 0.6637\n",
            "Epoch 63/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6100 - accuracy: 0.6480 - val_loss: 0.6049 - val_accuracy: 0.6637\n",
            "Epoch 64/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6096 - accuracy: 0.6509 - val_loss: 0.6045 - val_accuracy: 0.6637\n",
            "Epoch 65/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6096 - accuracy: 0.6509 - val_loss: 0.6042 - val_accuracy: 0.6637\n",
            "Epoch 66/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6069 - accuracy: 0.6519 - val_loss: 0.6101 - val_accuracy: 0.6726\n",
            "Epoch 67/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6094 - accuracy: 0.6480 - val_loss: 0.6041 - val_accuracy: 0.6637\n",
            "Epoch 68/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6088 - accuracy: 0.6519 - val_loss: 0.6047 - val_accuracy: 0.6637\n",
            "Epoch 69/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6084 - accuracy: 0.6490 - val_loss: 0.6056 - val_accuracy: 0.6726\n",
            "Epoch 70/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6082 - accuracy: 0.6480 - val_loss: 0.6057 - val_accuracy: 0.6726\n",
            "Epoch 71/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6079 - accuracy: 0.6470 - val_loss: 0.6031 - val_accuracy: 0.6637\n",
            "Epoch 72/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6069 - accuracy: 0.6480 - val_loss: 0.6048 - val_accuracy: 0.6726\n",
            "Epoch 73/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6080 - accuracy: 0.6529 - val_loss: 0.6033 - val_accuracy: 0.6637\n",
            "Epoch 74/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6077 - accuracy: 0.6500 - val_loss: 0.6036 - val_accuracy: 0.6726\n",
            "Epoch 75/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6073 - accuracy: 0.6529 - val_loss: 0.6032 - val_accuracy: 0.6726\n",
            "Epoch 76/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6071 - accuracy: 0.6519 - val_loss: 0.6027 - val_accuracy: 0.6637\n",
            "Epoch 77/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6067 - accuracy: 0.6529 - val_loss: 0.6022 - val_accuracy: 0.6637\n",
            "Epoch 78/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6064 - accuracy: 0.6549 - val_loss: 0.6048 - val_accuracy: 0.6726\n",
            "Epoch 79/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6061 - accuracy: 0.6519 - val_loss: 0.6032 - val_accuracy: 0.6726\n",
            "Epoch 80/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6060 - accuracy: 0.6500 - val_loss: 0.6019 - val_accuracy: 0.6637\n",
            "Epoch 81/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6065 - accuracy: 0.6509 - val_loss: 0.6025 - val_accuracy: 0.6726\n",
            "Epoch 82/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6061 - accuracy: 0.6450 - val_loss: 0.6017 - val_accuracy: 0.6637\n",
            "Epoch 83/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6057 - accuracy: 0.6480 - val_loss: 0.6015 - val_accuracy: 0.6637\n",
            "Epoch 84/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6047 - accuracy: 0.6490 - val_loss: 0.6019 - val_accuracy: 0.6726\n",
            "Epoch 85/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6058 - accuracy: 0.6549 - val_loss: 0.6015 - val_accuracy: 0.6726\n",
            "Epoch 86/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6050 - accuracy: 0.6529 - val_loss: 0.6040 - val_accuracy: 0.6814\n",
            "Epoch 87/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6050 - accuracy: 0.6500 - val_loss: 0.6013 - val_accuracy: 0.6637\n",
            "Epoch 88/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6051 - accuracy: 0.6568 - val_loss: 0.6012 - val_accuracy: 0.6637\n",
            "Epoch 89/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6050 - accuracy: 0.6490 - val_loss: 0.6015 - val_accuracy: 0.6726\n",
            "Epoch 90/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6041 - accuracy: 0.6529 - val_loss: 0.6047 - val_accuracy: 0.6814\n",
            "Epoch 91/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6052 - accuracy: 0.6549 - val_loss: 0.6020 - val_accuracy: 0.6726\n",
            "Epoch 92/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6048 - accuracy: 0.6490 - val_loss: 0.6009 - val_accuracy: 0.6726\n",
            "Epoch 93/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6047 - accuracy: 0.6509 - val_loss: 0.6009 - val_accuracy: 0.6726\n",
            "Epoch 94/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6038 - accuracy: 0.6539 - val_loss: 0.6021 - val_accuracy: 0.6726\n",
            "Epoch 95/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6045 - accuracy: 0.6480 - val_loss: 0.6005 - val_accuracy: 0.6726\n",
            "Epoch 96/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6041 - accuracy: 0.6470 - val_loss: 0.6009 - val_accuracy: 0.6726\n",
            "Epoch 97/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6031 - accuracy: 0.6509 - val_loss: 0.6044 - val_accuracy: 0.6637\n",
            "Epoch 98/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6039 - accuracy: 0.6539 - val_loss: 0.6026 - val_accuracy: 0.6726\n",
            "Epoch 99/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6039 - accuracy: 0.6421 - val_loss: 0.6003 - val_accuracy: 0.6726\n",
            "Epoch 100/100\n",
            "1017/1017 [==============================] - 2s 2ms/step - loss: 0.6038 - accuracy: 0.6509 - val_loss: 0.6002 - val_accuracy: 0.6726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o36CLXm_jHhZ",
        "colab": {}
      },
      "source": [
        "#load combined model\n",
        "meniscus_model = predictor()\n",
        "meniscus_model.load_weights('combine_transfer_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vYPyQ8vbjHhd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "21fbd957-3e78-4795-82d4-7530f380f561"
      },
      "source": [
        "# testing the model\n",
        "test_meniscus_sagittal_generator = data_generator('meniscus', 'sagittal', 'test')\n",
        "test_meniscus_coronal_generator = data_generator('meniscus', 'coronal', 'test')\n",
        "test_meniscus_axial_generator = data_generator('meniscus', 'axial', 'test')\n",
        "\n",
        "test_sagittal_prediction = meniscus_sagittal.predict_generator(test_meniscus_sagittal_generator)\n",
        "test_coronal_prediction = meniscus_coronal.predict_generator(test_meniscus_coronal_generator)\n",
        "test_axial_prediction = meniscus_axial.predict_generator(test_meniscus_axial_generator)\n",
        "\n",
        "test_input = tf.concat([test_sagittal_prediction, test_coronal_prediction, test_axial_prediction],1)\n",
        "test_output = tf.convert_to_tensor(get_output('meniscus','test'))\n",
        "\n",
        "meniscus_model.evaluate(test_input,test_output,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120/120 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.5583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6867029070854187, 0.5583333373069763]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm4vu_43xp3W",
        "colab_type": "text"
      },
      "source": [
        "#Contribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9FkkuFNSTT1",
        "colab_type": "text"
      },
      "source": [
        "##Abnormal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yiz2D61HSgEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sagittal_vgg = vgg_model()\n",
        "sagittal_resnet = ResNet()\n",
        "sagittal_inception = inception_v3()\n",
        "sagittal_vgg.load_weights('vgg_abnormal_sagittal.h5')\n",
        "sagittal_resnet.load_weights('resnet_abnormal_sagittal.h5')\n",
        "sagittal_inception.load_weights('inception_abnormal_sagittal.h5')\n",
        "\n",
        "coronal_vgg = vgg_model()\n",
        "coronal_resnet = ResNet()\n",
        "coronal_inception = inception_v3()\n",
        "coronal_vgg.load_weights('vgg_abnormal_coronal.h5')\n",
        "coronal_resnet.load_weights('resnet_abnormal_coronal.h5')\n",
        "coronal_inception.load_weights('inception_abnormal_coronal.h5')\n",
        "\n",
        "axial_vgg = vgg_model()\n",
        "axial_resnet = ResNet()\n",
        "axial_inception = inception_v3()\n",
        "axial_vgg.load_weights('vgg_abnormal_axial.h5')\n",
        "axial_resnet.load_weights('resnet_abnormal_axial.h5')\n",
        "axial_inception.load_weights('inception_abnormal_axial.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT8X9RKLSfpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_ sagittal_generator = data_generator('abnormal', 'sagittal','train')\n",
        "validation_sagittal_generator = data_generator('abnormal', 'sagittal','valid')\n",
        "\n",
        "training_vgg_prediction = sagittal_vgg.predict_generator(training_sagittal_generator)\n",
        "training_resnet_prediction = sagittal_resnet.predict_generator(training_sagittal_generator)\n",
        "training_inception_prediction = sagittal_inception.predict_generator(ttraining_sagittal_generator)\n",
        "\n",
        "validation_vgg_prediction = sagittal_vgg.predict_generator(validation_sagittal_generator)\n",
        "validation_resnet_prediction = sagittal_resnet.predict_generator(validation_sagittal_generator)\n",
        "validation_inception_prediction = sagittal_inception.predict_generator(validation_sagittal_generator)\n",
        "\n",
        "training_input = tf.concat([training_vgg_prediction, training_resnet_prediction, training_inception_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('abnormal','train'))\n",
        "validation_input = tf.concat([validation_vgg_prediction, validation_resnet_prediction, validation_inception_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('abnormal','valid'))\n",
        "\n",
        "sagittal_model = predictor()\n",
        "sagittal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "training_sagittal_prediction = sagittal_model.predict(training_input)\n",
        "validation_sagittal_prediction = sagittal_model.predict(validation_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkgk99hlSfRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_coronal_generator = data_generator('abnormal', 'coronal','train')\n",
        "validation_coronal_generator = data_generator('abnormal', 'coronal','valid')\n",
        "\n",
        "training_vgg_prediction = coronal_vgg.predict_generator(training_coronal_generator)\n",
        "training_resnet_prediction = coronal_resnet.predict_generator(training_coronal_generator)\n",
        "training_inception_prediction = coronal_inception.predict_generator(ttraining_coronal_generator)\n",
        "\n",
        "validation_vgg_prediction = coronal_vgg.predict_generator(validation_coronal_generator)\n",
        "validation_resnet_prediction = coronal_resnet.predict_generator(validation_coronal_generator)\n",
        "validation_inception_prediction = coronal_inception.predict_generator(validation_coronal_generator)\n",
        "\n",
        "training_input = tf.concat([training_vgg_prediction, training_resnet_prediction, training_inception_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('abnormal','train'))\n",
        "validation_input = tf.concat([validation_vgg_prediction, validation_resnet_prediction, validation_inception_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('abnormal','valid'))\n",
        "\n",
        "coronal_model = predictor()\n",
        "coronal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "training_coronal_prediction = coronal_model.predict(training_input)\n",
        "validation_coronal_prediction = coronal_model.predict(validation_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3QD4f09Se28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_axial_generator = data_generator('abnormal', 'axial','train')\n",
        "validation_axial_generator = data_generator('abnormal', 'axial','valid')\n",
        "\n",
        "training_vgg_prediction = axial_vgg.predict_generator(training_axial_generator)\n",
        "training_resnet_prediction = axial_resnet.predict_generator(training_axial_generator)\n",
        "training_inception_prediction = axial_inception.predict_generator(ttraining_axial_generator)\n",
        "\n",
        "validation_vgg_prediction = axial_vgg.predict_generator(validation_axial_generator)\n",
        "validation_resnet_prediction = axial_resnet.predict_generator(validation_axial_generator)\n",
        "validation_inception_prediction = axial_inception.predict_generator(validation_axial_generator)\n",
        "\n",
        "training_input = tf.concat([training_vgg_prediction, training_resnet_prediction, training_inception_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('abnormal','train'))\n",
        "validation_input = tf.concat([validation_vgg_prediction, validation_resnet_prediction, validation_inception_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('abnormal','valid'))\n",
        "\n",
        "axial_model = predictor()\n",
        "axial_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "training_axial_prediction = axial_model.predict(training_input)\n",
        "validation_axial_prediction = axial_model.predict(validation_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz8kEJbvSeai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('abnormal','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('abnormal','valid'))\n",
        "\n",
        "model = predictor()\n",
        "model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "model.save('contribution_abnormal.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzzeGfGFSX2H",
        "colab_type": "text"
      },
      "source": [
        "##ACL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUFR-S7vTCPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sagittal_vgg = vgg_model()\n",
        "sagittal_resnet = ResNet()\n",
        "sagittal_inception = inception_v3()\n",
        "sagittal_vgg.load_weights('vgg_acl_sagittal.h5')\n",
        "sagittal_resnet.load_weights('resnet_acl_sagittal.h5')\n",
        "sagittal_inception.load_weights('inception_acl_sagittal.h5')\n",
        "\n",
        "coronal_vgg = vgg_model()\n",
        "coronal_resnet = ResNet()\n",
        "coronal_inception = inception_v3()\n",
        "coronal_vgg.load_weights('vgg_acl_coronal.h5')\n",
        "coronal_resnet.load_weights('resnet_acl_coronal.h5')\n",
        "coronal_inception.load_weights('inception_acl_coronal.h5')\n",
        "\n",
        "axial_vgg = vgg_model()\n",
        "axial_resnet = ResNet()\n",
        "axial_inception = inception_v3()\n",
        "axial_vgg.load_weights('vgg_acl_axial.h5')\n",
        "axial_resnet.load_weights('resnet_acl_axial.h5')\n",
        "axial_inception.load_weights('inception_acl_axial.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4or752VTB7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_ sagittal_generator = data_generator('acl', 'sagittal','train')\n",
        "validation_sagittal_generator = data_generator('acl', 'sagittal','valid')\n",
        "\n",
        "training_vgg_prediction = sagittal_vgg.predict_generator(training_sagittal_generator)\n",
        "training_resnet_prediction = sagittal_resnet.predict_generator(training_sagittal_generator)\n",
        "training_inception_prediction = sagittal_inception.predict_generator(ttraining_sagittal_generator)\n",
        "\n",
        "validation_vgg_prediction = sagittal_vgg.predict_generator(validation_sagittal_generator)\n",
        "validation_resnet_prediction = sagittal_resnet.predict_generator(validation_sagittal_generator)\n",
        "validation_inception_prediction = sagittal_inception.predict_generator(validation_sagittal_generator)\n",
        "\n",
        "training_input = tf.concat([training_vgg_prediction, training_resnet_prediction, training_inception_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('acl','train'))\n",
        "validation_input = tf.concat([validation_vgg_prediction, validation_resnet_prediction, validation_inception_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('acl','valid'))\n",
        "\n",
        "sagittal_model = predictor()\n",
        "sagittal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "training_sagittal_prediction = sagittal_model.predict(training_input)\n",
        "validation_sagittal_prediction = sagittal_model.predict(validation_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncAhdrsqTBd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_coronal_generator = data_generator('acl', 'coronal','train')\n",
        "validation_coronal_generator = data_generator('acl', 'coronal','valid')\n",
        "\n",
        "training_vgg_prediction = coronal_vgg.predict_generator(training_coronal_generator)\n",
        "training_resnet_prediction = coronal_resnet.predict_generator(training_coronal_generator)\n",
        "training_inception_prediction = coronal_inception.predict_generator(ttraining_coronal_generator)\n",
        "\n",
        "validation_vgg_prediction = coronal_vgg.predict_generator(validation_coronal_generator)\n",
        "validation_resnet_prediction = coronal_resnet.predict_generator(validation_coronal_generator)\n",
        "validation_inception_prediction = coronal_inception.predict_generator(validation_coronal_generator)\n",
        "\n",
        "training_input = tf.concat([training_vgg_prediction, training_resnet_prediction, training_inception_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('acl','train'))\n",
        "validation_input = tf.concat([validation_vgg_prediction, validation_resnet_prediction, validation_inception_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('acl','valid'))\n",
        "\n",
        "coronal_model = predictor()\n",
        "coronal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "training_coronal_prediction = coronal_model.predict(training_input)\n",
        "validation_coronal_prediction = coronal_model.predict(validation_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAJ1uv44TBMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_axial_generator = data_generator('acl', 'axial','train')\n",
        "validation_axial_generator = data_generator('acl', 'axial','valid')\n",
        "\n",
        "training_vgg_prediction = axial_vgg.predict_generator(training_axial_generator)\n",
        "training_resnet_prediction = axial_resnet.predict_generator(training_axial_generator)\n",
        "training_inception_prediction = axial_inception.predict_generator(ttraining_axiall_generator)\n",
        "\n",
        "validation_vgg_prediction = axial_vgg.predict_generator(validation_axial_generator)\n",
        "validation_resnet_prediction = axial_resnet.predict_generator(validation_axial_generator)\n",
        "validation_inception_prediction = axial_inception.predict_generator(validation_axiall_generator)\n",
        "\n",
        "training_input = tf.concat([training_vgg_prediction, training_resnet_prediction, training_inception_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('acl','train'))\n",
        "validation_input = tf.concat([validation_vgg_prediction, validation_resnet_prediction, validation_inception_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('acl','valid'))\n",
        "\n",
        "axial_model = predictor()\n",
        "axial_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "training_axial_prediction = axial_model.predict(training_input)\n",
        "validation_axial_prediction = axial_model.predict(validation_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbjKX5ksTAv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('acl','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('acl','valid'))\n",
        "\n",
        "model = predictor()\n",
        "model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "model.save('contribution_acl.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDkG8HzFSahi",
        "colab_type": "text"
      },
      "source": [
        "##Meniscus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRC7Uvp-xpL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sagittal_vgg = vgg_model()\n",
        "sagittal_resnet = ResNet()\n",
        "sagittal_inception = inception_v3()\n",
        "sagittal_vgg.load_weights('vgg_meniscus_sagittal.h5')\n",
        "sagittal_resnet.load_weights('resnet_meniscus_sagittal.h5')\n",
        "sagittal_inception.load_weights('inception_meniscus_sagittal.h5')\n",
        "\n",
        "coronal_vgg = vgg_model()\n",
        "coronal_resnet = ResNet()\n",
        "coronal_inception = inception_v3()\n",
        "coronal_vgg.load_weights('vgg_meniscus_coronal.h5')\n",
        "coronal_resnet.load_weights('resnet_meniscus_coronal.h5')\n",
        "coronal_inception.load_weights('inception_meniscus_coronal.h5')\n",
        "\n",
        "axial_vgg = vgg_model()\n",
        "axial_resnet = ResNet()\n",
        "axial_inception = inception_v3()\n",
        "axial_vgg.load_weights('vgg_meniscus_axial.h5')\n",
        "axial_resnet.load_weights('resnet_meniscus_axial.h5')\n",
        "axial_inception.load_weights('inception_meniscus_axial.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWveDTMuTxfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_ sagittal_generator = data_generator('meniscus', 'sagittal','train')\n",
        "validation_sagittal_generator = data_generator('meniscus', 'sagittal','valid')\n",
        "\n",
        "training_vgg_prediction = sagittal_vgg.predict_generator(training_sagittal_generator)\n",
        "training_resnet_prediction = sagittal_resnet.predict_generator(training_sagittal_generator)\n",
        "training_inception_prediction = sagittal_inception.predict_generator(ttraining_sagittal_generator)\n",
        "\n",
        "validation_vgg_prediction = sagittal_vgg.predict_generator(validation_sagittal_generator)\n",
        "validation_resnet_prediction = sagittal_resnet.predict_generator(validation_sagittal_generator)\n",
        "validation_inception_prediction = sagittal_inception.predict_generator(validation_sagittal_generator)\n",
        "\n",
        "training_input = tf.concat([training_vgg_prediction, training_resnet_prediction, training_inception_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('meniscus','train'))\n",
        "validation_input = tf.concat([validation_vgg_prediction, validation_resnet_prediction, validation_inception_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('meniscus','valid'))\n",
        "\n",
        "sagittal_model = predictor()\n",
        "sagittal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "training_sagittal_prediction = sagittal_model.predict(training_input)\n",
        "validation_sagittal_prediction = sagittal_model.predict(validation_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAYSrG4xTwgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_coronal_generator = data_generator('meniscus', 'coronal','train')\n",
        "validation_coronal_generator = data_generator('meniscus', 'coronal','valid')\n",
        "\n",
        "training_vgg_prediction = coronal_vgg.predict_generator(training_coronal_generator)\n",
        "training_resnet_prediction = coronal_resnet.predict_generator(training_coronal_generator)\n",
        "training_inception_prediction = coronal_inception.predict_generator(ttraining_coronal_generator)\n",
        "\n",
        "validation_vgg_prediction = coronal_vgg.predict_generator(validation_coronal_generator)\n",
        "validation_resnet_prediction = coronal_resnet.predict_generator(validation_coronal_generator)\n",
        "validation_inception_prediction = coronal_inception.predict_generator(validation_coronal_generator)\n",
        "\n",
        "training_input = tf.concat([training_vgg_prediction, training_resnet_prediction, training_inception_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('meniscus','train'))\n",
        "validation_input = tf.concat([validation_vgg_prediction, validation_resnet_prediction, validation_inception_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('meniscus','valid'))\n",
        "\n",
        "coronal_model = predictor()\n",
        "coronal_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "training_coronal_prediction = coronal_model.predict(training_input)\n",
        "validation_coronal_prediction = coronal_model.predict(validation_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XSFR7cUTvxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_axial_generator = data_generator('meniscus', 'axial','train')\n",
        "validation_axial_generator = data_generator('meniscus', 'axial','valid')\n",
        "\n",
        "training_vgg_prediction = axial_vgg.predict_generator(training_axial_generator)\n",
        "training_resnet_prediction = axial_resnet.predict_generator(training_axial_generator)\n",
        "training_inception_prediction = axial_inception.predict_generator(ttraining_axiall_generator)\n",
        "\n",
        "validation_vgg_prediction = axial_vgg.predict_generator(validation_axial_generator)\n",
        "validation_resnet_prediction = axial_resnet.predict_generator(validation_axial_generator)\n",
        "validation_inception_prediction = axial_inception.predict_generator(validation_axiall_generator)\n",
        "\n",
        "training_input = tf.concat([training_vgg_prediction, training_resnet_prediction, training_inception_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('meniscus','train'))\n",
        "validation_input = tf.concat([validation_vgg_prediction, validation_resnet_prediction, validation_inception_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('meniscus','valid'))\n",
        "\n",
        "axial_model = predictor()\n",
        "axial_model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "\n",
        "training_axial_prediction = axial_model.predict(training_input)\n",
        "validation_axial_prediction = axial_model.predict(validation_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRdUeuIsTu3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_input = tf.concat([training_sagittal_prediction, training_coronal_prediction, training_axial_prediction],1)\n",
        "training_output = tf.convert_to_tensor(get_output('meniscus','train'))\n",
        "validation_input = tf.concat([validation_sagittal_prediction, validation_coronal_prediction, validation_axial_prediction],1)\n",
        "validation_output = tf.convert_to_tensor(get_output('meniscus','valid'))\n",
        "\n",
        "model = predictor()\n",
        "model.fit(training_input, training_output,epochs=30,validation_data=(validation_input,validation_output),batch_size=1)\n",
        "model.save('contribution_meniscus.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UV6pTeAiv9D",
        "colab_type": "text"
      },
      "source": [
        "#Failed attempt 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObsTm8MEHHhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data generator\n",
        "class data_generator_multiple(keras.utils.Sequence):\n",
        "    def __init__(self, injury, data_type):\n",
        "        self.injury = injury\n",
        "        self.data_type = data_type\n",
        "        if(data_type == 'train'):\n",
        "          self.labels = pd.read_csv('train-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "          self.case_list = pd.read_csv('train-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                               dtype={'case': str, 'label': np.int64})['case'].tolist()\n",
        "          self.labels = self.labels[0:1017]\n",
        "          self.case_list = self.case_list[0:1017]\n",
        "          self.data_path = 'train'\n",
        "        elif(data_type == 'valid'):\n",
        "          self.labels = pd.read_csv('train-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "          self.case_list = pd.read_csv('train-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                               dtype={'case': str, 'label': np.int64})['case'].tolist()\n",
        "          self.labels = self.labels[1017:1130]\n",
        "          self.case_list = self.case_list[1017:1130]\n",
        "          self.data_path = 'train'\n",
        "        else:\n",
        "          self.labels = pd.read_csv('valid-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "          self.case_list = pd.read_csv('valid-{}.csv'.format(self.injury), names=['case', 'label'], header=None,\n",
        "                               dtype={'case': str, 'label': np.int64})['case'].tolist()\n",
        "          self.data_path = 'valid'\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.case_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fpath = '{}/{}/{}.npy'.format(self.data_path, 'sagittal', self.case_list[index])\n",
        "        stack1 = np.load(fpath).astype(float)\n",
        "        for i in range(stack1.shape[0]):\n",
        "          stack1[i] = stack1[i]/255.\n",
        "        stack1 = np.repeat(stack1[:, :, :, np.newaxis], 3, axis=3)\n",
        "        stack1 = np.expand_dims(stack1, axis=0)\n",
        "\n",
        "        fpath = '{}/{}/{}.npy'.format(self.data_path, 'coronal', self.case_list[index])\n",
        "        stack2 = np.load(fpath).astype(float)\n",
        "        for i in range(stack2.shape[0]):\n",
        "          stack2[i] = stack2[i]/255.\n",
        "        stack2 = np.repeat(stack2[:, :, :, np.newaxis], 3, axis=3)\n",
        "        stack2 = np.expand_dims(stack2, axis=0)\n",
        "\n",
        "        fpath = '{}/{}/{}.npy'.format(self.data_path, 'axial', self.case_list[index])\n",
        "        stack3 = np.load(fpath).astype(float)\n",
        "        for i in range(stack3.shape[0]):\n",
        "          stack3[i] = stack3[i]/255.\n",
        "        stack3 = np.repeat(stack3[:, :, :, np.newaxis], 3, axis=3)\n",
        "        stack3 = np.expand_dims(stack3, axis=0)\n",
        "\n",
        "\n",
        "        y = self.labels[index]\n",
        "        y = np.array(y).reshape(1,1)\n",
        "        yield(stack1, stack2, stack3, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR1_vS5cgEa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_one_stack(case, data_path, plane='coronal'):\n",
        "    fpath = '{}/{}/{}.npy'.format(data_path, plane, case)\n",
        "    stack = np.load(fpath).astype(float)\n",
        "    for i in range(stack.shape[0]):\n",
        "      stack[i] = stack[i]/255.\n",
        "\n",
        "    return np.repeat(stack[:, :, :, np.newaxis], 3, axis=3)\n",
        "\n",
        "def load_stacks(case, data_path):\n",
        "    x = {}\n",
        "    planes = ['coronal', 'sagittal', 'axial']\n",
        "    for i, plane in enumerate(planes):\n",
        "\n",
        "        x[plane] = load_one_stack(case, data_path, plane=plane)\n",
        "    return x\n",
        "\n",
        "def load_cases(train=True, n=30, index=0):\n",
        "    assert (type(n) == int) and (n < 1250)\n",
        "    if train:\n",
        "        case_list = pd.read_csv('train-acl.csv', names=['case', 'label'], header=None,\n",
        "                               dtype={'case': str, 'label': np.int64})['case'].tolist()\n",
        "        y_acl =  pd.read_csv('train-acl.csv', names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "        y_meniscus =  pd.read_csv('train-meniscus.csv', names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "        y_abnormal =  pd.read_csv('train-abnormal.csv', names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()       \n",
        "    else:\n",
        "        case_list = pd.read_csv('valid-acl.csv', names=['case', 'label'], header=None,\n",
        "                               dtype={'case': str, 'label': np.int64})['case'].tolist() \n",
        "        y_acl =  pd.read_csv('valid-acl.csv', names=['case', 'label'], header=None,\n",
        "                               dtype={'case': str, 'label': np.int64})['label'].tolist()                           \n",
        "        y_meniscus =  pd.read_csv('valid-meniscus.csv', names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "        y_abnormal =  pd.read_csv('valid-abnormal.csv', names=['case', 'label'], header=None,\n",
        "                              dtype={'case': str, 'label': np.int64})['label'].tolist()\n",
        "    cases = {}\n",
        "    if n is not None:\n",
        "        case_list = case_list[index*n : (index+1)*n]\n",
        "        y_acl = y_acl[index*n : (index+1)*n]\n",
        "        y_meniscus = y_meniscus[index*n : (index+1)*n]\n",
        "        y_abnormal = y_abnormal[index*n : (index+1)*n]\n",
        "        \n",
        "    for case in tqdm_notebook(case_list, leave=False):\n",
        "      if train:\n",
        "        x = load_stacks(case, 'train')\n",
        "      else: \n",
        "        x = load_stacks(case, 'valid')\n",
        "        \n",
        "      cases[case] = x\n",
        "    \n",
        "    percent_train = (int)(0.9*len(cases))\n",
        "    if train:\n",
        "      return dict(list(cases.items())[:percent_train]), dict(list(cases.items())[percent_train:]) ,\n",
        "      y_acl, y_meniscus, y_abnormal\n",
        "    else:\n",
        "      return cases , y_acl, y_meniscus, y_abnormal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3igV_em_3WD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def free_ram(x_train, x_valid):\n",
        "  del x_train\n",
        "  del x_valid\n",
        "\n",
        "# free_ram()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3Xf61e1iRAO",
        "colab_type": "text"
      },
      "source": [
        "#Failed attempt 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34DhGFDmjDrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_axial_abnormal = models.Sequential()\n",
        "model_axial_abnormal.add(layers.Flatten(input_shape=[1, 1, 512]))\n",
        "model_axial_abnormal.add(layers.Dense(512, activation='relu'))\n",
        "model_axial_abnormal.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_axial_abnormal.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\n",
        "#model_axial_abnormal.summary()\n",
        "\n",
        "model_coronal_abnormal = models.Sequential()\n",
        "model_coronal_abnormal.add(layers.Flatten(input_shape=[1, 1, 512]))\n",
        "model_coronal_abnormal.add(layers.Dense(512, activation='relu'))\n",
        "model_coronal_abnormal.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_coronal_abnormal.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\n",
        "#model_coronal_abnormal.summary()\n",
        "\n",
        "model_sagittal_abnormal = models.Sequential()\n",
        "model_sagittal_abnormal.add(layers.Flatten(input_shape=[1, 1, 512]))\n",
        "model_sagittal_abnormal.add(layers.Dense(512, activation='relu'))\n",
        "model_sagittal_abnormal.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_sagittal_abnormal.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\n",
        "#model_sagittal_abnormal.summary()\n",
        "\n",
        "model_abnormal = models.Sequential()\n",
        "model_abnormal.add(keras.Input(shape=(3,)))\n",
        "model_abnormal.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_abnormal.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\n",
        "#model_abnormal.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojkp65pAsJhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model Training\n",
        "n = 30\n",
        "percent_train = (int)(0.9*n)\n",
        "\n",
        "for i in range (20):\n",
        "  print (i)\n",
        "\n",
        "  #data generation\n",
        "  x_train, x_valid, y_train_acl, y_train_meniscus, y_train_abnormal = load_cases(index=i)\n",
        "  y_train_acl, y_valid_acl = y_train_acl[:percent_train], y_train_acl[percent_train:]\n",
        "  y_train_meniscus, y_valid_meniscus = y_train_meniscus[:percent_train], y_train_meniscus[percent_train:]\n",
        "  y_train_abnormal, y_valid_abnormal = y_train_abnormal[:percent_train], y_train_abnormal[percent_train:]\n",
        "\n",
        "  #prediction\n",
        "  x_train_vgg_axial = []\n",
        "  x_train_vgg_coronal = []\n",
        "  x_train_vgg_sagittal = []\n",
        "  for case in x_train:\n",
        "    x_train_vgg_axial.append(tf.reduce_max(VGGmodel.predict(x_train[case]['axial']),0))\n",
        "    x_train_vgg_coronal.append(tf.reduce_max(VGGmodel.predict(x_train[case]['coronal']),0))\n",
        "    x_train_vgg_sagittal.append(tf.reduce_max(VGGmodel.predict(x_train[case]['sagittal']),0))\n",
        "\n",
        "  x_valid_vgg_axial = []\n",
        "  x_valid_vgg_coronal = []\n",
        "  x_valid_vgg_sagittal = []\n",
        "  for case in x_valid:\n",
        "    x_valid_vgg_axial.append(tf.reduce_max(VGGmodel.predict(x_valid[case]['axial']),0))\n",
        "    x_valid_vgg_coronal.append(tf.reduce_max(VGGmodel.predict(x_valid[case]['coronal']),0))\n",
        "    x_valid_vgg_sagittal.append(tf.reduce_max(VGGmodel.predict(x_valid[case]['sagittal']),0))\n",
        "\n",
        "  #convert to tensor\n",
        "  x_train_vgg_axial = tf.convert_to_tensor(x_train_vgg_axial)\n",
        "  x_valid_vgg_axial = tf.convert_to_tensor(x_valid_vgg_axial)\n",
        "  x_train_vgg_coronal = tf.convert_to_tensor(x_train_vgg_coronal)\n",
        "  x_valid_vgg_coronal = tf.convert_to_tensor(x_valid_vgg_coronal)\n",
        "  x_train_vgg_sagittal = tf.convert_to_tensor(x_train_vgg_sagittal)\n",
        "  x_valid_vgg_sagittal = tf.convert_to_tensor(x_valid_vgg_sagittal)\n",
        "\n",
        "  y_train_abnormal = tf.convert_to_tensor(y_train_abnormal)\n",
        "  y_valid_abnormal = tf.convert_to_tensor(y_valid_abnormal)\n",
        "\n",
        "  #train seperate models\n",
        "  #model_axial_abnormal.history = model_axial_abnormal.fit(x_train_vgg_axial,y_train_abnormal,epochs=30,validation_data=(x_valid_vgg_axial,y_valid_abnormal))\n",
        "  #model_coronal_abnormal.history = model_coronal_abnormal.fit(x_train_vgg_coronal,y_train_abnormal,epochs=30,validation_data=(x_valid_vgg_coronal,y_valid_abnormal))\n",
        "  #model_sagittal_abnormal.history = model_sagittal_abnormal.fit(x_train_vgg_sagittal,y_train_abnormal,epochs=30,validation_data=(x_valid_vgg_sagittal,y_valid_abnormal))\n",
        "  \n",
        "  #train final model\n",
        "  train_abnormal = model_axial_abnormal.predict(x_train_vgg_axial)\n",
        "  train_abnormal = np.c_[train_abnormal,model_coronal_abnormal.predict(x_train_vgg_coronal)]\n",
        "  train_abnormal = np.c_[train_abnormal,model_sagittal_abnormal.predict(x_train_vgg_sagittal)]\n",
        "\n",
        "  valid_abnormal = model_axial_abnormal.predict(x_valid_vgg_axial)\n",
        "  valid_abnormal = np.c_[valid_abnormal,model_coronal_abnormal.predict(x_valid_vgg_coronal)]\n",
        "  valid_abnormal = np.c_[valid_abnormal,model_sagittal_abnormal.predict(x_valid_vgg_sagittal)]\n",
        "\n",
        "  train_abnormal = tf.convert_to_tensor(train_abnormal)\n",
        "  valid_abnormal = tf.convert_to_tensor(valid_abnormal)\n",
        "\n",
        "  model_abnormal.history = model_abnormal.fit(train_abnormal,y_train_abnormal,epochs=30,validation_data=(valid_abnormal,y_valid_abnormal))\n",
        "  \n",
        "  free_ram(x_train, x_valid)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee5x2L_3uvhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#testing the trained model\n",
        "test_size = 4\n",
        "abnormal_results = []\n",
        "\n",
        "for i in range (test_size):\n",
        "  x_test, y_test_acl, y_test_meniscus, y_test_abnormal = load_cases(False, index=i)\n",
        "\n",
        "  x_test_vgg_axial = []\n",
        "  x_test_vgg_coronal = []\n",
        "  x_test_vgg_sagittal = []\n",
        "  for case in x_test:\n",
        "    x_test_vgg_axial.append(tf.reduce_max(VGGmodel.predict(x_test[case]['axial']),0))\n",
        "    x_test_vgg_coronal.append(tf.reduce_max(VGGmodel.predict(x_test[case]['coronal']),0))\n",
        "    x_test_vgg_sagittal.append(tf.reduce_max(VGGmodel.predict(x_test[case]['sagittal']),0))\n",
        "\n",
        "  del x_test\n",
        "\n",
        "  x_test_vgg_axial = tf.convert_to_tensor(x_test_vgg_axial)\n",
        "  x_test_vgg_coronal = tf.convert_to_tensor(x_test_vgg_coronal)\n",
        "  x_test_vgg_sagittal = tf.convert_to_tensor(x_test_vgg_sagittal)\n",
        "\n",
        "  y_test_abnormal = tf.convert_to_tensor(y_test_abnormal)\n",
        "\n",
        "  test_abnormal = model_axial_abnormal.predict(x_test_vgg_axial)\n",
        "  test_abnormal = np.c_[test_abnormal,model_coronal_abnormal.predict(x_test_vgg_coronal)]\n",
        "  test_abnormal = np.c_[test_abnormal,model_sagittal_abnormal.predict(x_test_vgg_sagittal)]\n",
        "\n",
        "  test_abnormal = tf.convert_to_tensor(test_abnormal)\n",
        "\n",
        "  result = model_abnormal.evaluate(test_abnormal, y_test_abnormal)\n",
        "  abnormal_results.append(result)\n",
        "\n",
        "print(np.mean(abnormal_results, axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtSmTjdZUc7z",
        "colab_type": "text"
      },
      "source": [
        "#Failed attempt 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob3t7QRMUXZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy3xtDBEubm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import keras\n",
        "# from keras.models import Model\n",
        "# from keras.layers import (\n",
        "#     Input,\n",
        "#     Dense,\n",
        "#     Flatten,\n",
        "#     Activation,\n",
        "#     GlobalMaxPooling3D\n",
        "# )\n",
        "# from keras.layers.convolutional import (\n",
        "#     Conv3D,\n",
        "#     MaxPooling3D,\n",
        "#     AveragePooling3D\n",
        "# )\n",
        "# from keras.layers.normalization import BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtMSgohli7NF",
        "colab_type": "text"
      },
      "source": [
        "### **Building Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr60VxT-tL7g",
        "colab_type": "text"
      },
      "source": [
        "**use keras built-in Inception V3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GmVQV21USdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "con_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(256,256,3))\n",
        "con_base.summary()\n",
        "\n",
        "\n",
        "inception_model = models.Sequential()\n",
        "\n",
        "#con_base output_shape = s*6*6*2048\n",
        "inception_model.add(con_base)\n",
        "\n",
        "#after appling global average pooling, output_shape = s*2048\n",
        "inception_model.add(layers.AveragePooling2D(6,6))\n",
        "\n",
        "con_base.trainable = False\n",
        "\n",
        "model_axial_abnormal = models.Sequential()\n",
        "model_axial_abnormal.add(layers.Flatten(input_shape=[1, 1, 2048]))\n",
        "model_axial_abnormal.add(layers.Dense(2048, activation='relu'))\n",
        "model_axial_abnormal.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model_axial_abnormal.summary()\n",
        "\n",
        "model_coronal_abnormal = models.Sequential()\n",
        "model_coronal_abnormal.add(layers.Flatten(input_shape=[1, 1, 2048]))\n",
        "model_coronal_abnormal.add(layers.Dense(2048, activation='relu'))\n",
        "model_coronal_abnormal.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model_coronal_abnormal.summary()\n",
        "\n",
        "model_sagittal_abnormal = models.Sequential()\n",
        "model_sagittal_abnormal.add(layers.Flatten(input_shape=[1, 1, 2048]))\n",
        "model_sagittal_abnormal.add(layers.Dense(2048, activation='relu'))\n",
        "model_sagittal_abnormal.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model_sagittal_abnormal.summary()\n",
        "\n",
        "LR_model = models.Sequential()\n",
        "LR_model.add(layers.Dense(1, activation='sigmoid',input_shape = [3]))\n",
        "\n",
        "#LR_model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_HOgX_Tj98p",
        "colab_type": "text"
      },
      "source": [
        "### **Training models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nv3RwX3kfL3",
        "colab_type": "text"
      },
      "source": [
        "**pass to inceptionV3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5ZYPWX_s1SJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reduce_max => perform max pooling across slices to obtain a 2048-dimensional vector\n",
        "x_train_inc_axial = []\n",
        "x_train_inc_coronal = []\n",
        "x_train_inc_sagittal = []\n",
        "for case in x_train:\n",
        "  x_train_inc_axial.append(tf.reduce_max(inception_model.predict(x_train[case]['axial']),0))\n",
        "  x_train_inc_coronal.append(tf.reduce_max(inception_model.predict(x_train[case]['coronal']),0))\n",
        "  x_train_inc_sagittal.append(tf.reduce_max(inception_model.predict(x_train[case]['sagittal']),0))\n",
        "\n",
        "x_valid_inc_axial = []\n",
        "x_valid_inc_coronal = []\n",
        "x_valid_inc_sagittal = []\n",
        "for case in x_valid:\n",
        "  x_valid_inc_axial.append(tf.reduce_max(inception_model.predict(x_valid[case]['axial']),0))\n",
        "  x_valid_inc_coronal.append(tf.reduce_max(inception_model.predict(x_valid[case]['coronal']),0))\n",
        "  x_valid_inc_sagittal.append(tf.reduce_max(inception_model.predict(x_valid[case]['sagittal']),0))\n",
        "\n",
        "x_test_inc_axial = []\n",
        "x_test_inc_coronal = []\n",
        "x_test_inc_sagittal = []\n",
        "for case in x_test:\n",
        "  x_test_inc_axial.append(tf.reduce_max(inception_model.predict(x_test[case]['axial']),0))\n",
        "  x_test_inc_coronal.append(tf.reduce_max(inception_model.predict(x_test[case]['coronal']),0))\n",
        "  x_test_inc_sagittal.append(tf.reduce_max(inception_model.predict(x_test[case]['sagittal']),0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7emwG7JTsw9w",
        "colab": {}
      },
      "source": [
        "x_train_inc_axial = tf.convert_to_tensor(x_train_inc_axial)\n",
        "x_valid_inc_axial = tf.convert_to_tensor(x_valid_inc_axial)\n",
        "x_test_inc_axial = tf.convert_to_tensor(x_test_inc_axial)\n",
        "\n",
        "x_train_inc_coronal = tf.convert_to_tensor(x_train_inc_coronal)\n",
        "x_valid_inc_coronal = tf.convert_to_tensor(x_valid_inc_coronal)\n",
        "x_test_inc_coronal = tf.convert_to_tensor(x_test_inc_coronal)\n",
        "\n",
        "x_train_inc_sagittal = tf.convert_to_tensor(x_train_inc_sagittal)\n",
        "x_valid_inc_sagittal = tf.convert_to_tensor(x_valid_inc_sagittal)\n",
        "x_test_inc_sagittal = tf.convert_to_tensor(x_test_inc_sagittal)\n",
        "\n",
        "y_train_abnormal = tf.convert_to_tensor(y_train_abnormal)\n",
        "y_valid_abnormal = tf.convert_to_tensor(y_valid_abnormal)\n",
        "y_test_abnormal = tf.convert_to_tensor(y_test_abnormal)\n",
        "print(x_train_inc_axial[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Ki30wbLCmU",
        "colab_type": "text"
      },
      "source": [
        "**pass to fully connected layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUpQdJwM5rMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_axial_abnormal.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "model_axial_abnormal.history = model_axial_abnormal.fit(x_train_inc_axial,y_train_abnormal,epochs=30,validation_data=(x_valid_inc_axial,y_valid_abnormal))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akgkysAoJkP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test accuracy\n",
        "model_axial_abnormal.predict(x_test_inc_axial,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0nM-gkZLw54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_coronal_abnormal.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "model_coronal_abnormal.history = model_coronal_abnormal.fit(x_train_inc_coronal,y_train_abnormal,epochs=30,validation_data=(x_valid_inc_coronal,y_valid_abnormal))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxPyKIchLxjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test accuracy\n",
        "model_coronal_abnormal.predict(x_test_inc_coronal, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5BD-esfLx92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_sagittal_abnormal.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "model_sagittal_abnormal.history = model_sagittal_abnormal.fit(x_train_inc_sagittal,y_train_abnormal,epochs=30,validation_data=(x_valid_inc_sagittal,y_valid_abnormal))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_j_TJzRLyV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test accuracy\n",
        "model_sagittal_abnormal.predict(x_test_inc_sagittal, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEzxNcRPlMX-",
        "colab_type": "text"
      },
      "source": [
        "**pass to logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hZKuRwuPk8e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80fee3f4-511e-4414-dfe4-f74e8a3ffd26"
      },
      "source": [
        "axial_inc_prediction_t = model_axial_abnormal.predict(x_train_inc_axial,0)\n",
        "coronal_inc_prediction_t = model_axial_abnormal.predict(x_train_inc_coronal,0)\n",
        "sagittal_inc_prediction_t = model_axial_abnormal.predict(x_train_inc_sagittal,0)\n",
        "\n",
        "LR_train = tf.concat([axial_inc_prediction_t, coronal_inc_prediction_t, sagittal_inc_prediction_t],1)\n",
        "print(LR_train.shape)\n",
        "\n",
        "axial_inc_prediction_v = model_axial_abnormal.predict(x_valid_inc_axial,0)\n",
        "coronal_inc_prediction_v = model_axial_abnormal.predict(x_valid_inc_coronal,0)\n",
        "sagittal_inc_prediction_v = model_axial_abnormal.predict(x_valid_inc_sagittal,0)\n",
        "\n",
        "LR_vaild = tf.concat([axial_inc_prediction_v, coronal_inc_prediction_v, sagittal_inc_prediction_v],1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUM4VnhNmqog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR_model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "LR_model.history = LR_model.fit(LR_train,y_train_abnormal,epochs=60,validation_data=(LR_vaild,y_valid_abnormal))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}